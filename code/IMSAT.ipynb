{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.init as init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nData Preprocessing\\n\\n'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Data Preprocessing\n",
    "\n",
    "'''\n",
    "#TODO Preprocess the data to a suitable format\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nSettings\\n\\n'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Settings\n",
    "\n",
    "'''\n",
    "#TODO Define the settings of the IMAST model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Deep Neural Network\n",
    "\n",
    "'''\n",
    "\n",
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        \n",
    "        # Add first fully connected layer with 28 * 28 = 784 input neurons and 1200 output neurons\n",
    "        self.fc1 = nn.Linear(28 * 28, 1200)\n",
    "        # Initialize the weights of the first fully connected layer using the He normal initialization\n",
    "        init.kaiming_normal_(self.fc1.weight, nonlinearity='relu')\n",
    "        # Add first batch normalization layer with 1200 neurons and epsilon = 2e-5\n",
    "        self.bn1   = nn.BatchNorm1d(1200, eps=2e-5)\n",
    "        self.bn1_F = nn.BatchNorm1d(1200, eps=2e-5, affine=False)\n",
    "        # Add first ReLU activation function\n",
    "        self.relu1 = nn.ReLU()\n",
    "        \n",
    "        self.fc2 = nn.Linear(1200, 1200)\n",
    "        init.kaiming_normal_(self.fc2.weight, nonlinearity='relu')\n",
    "        self.bn2   = nn.BatchNorm1d(1200, eps=2e-5)\n",
    "        self.bn2_F = nn.BatchNorm1d(1200, eps=2e-5, affine=False)\n",
    "\n",
    "        self.relu2 = nn.ReLU()\n",
    "        \n",
    "        # Add output layer of size 10 \n",
    "        self.fc3 = nn.Linear(1200, 10)\n",
    "        init.kaiming_normal_(self.fc3.weight, nonlinearity='linear')\n",
    "        \n",
    "    # Define the forward pass through the network\n",
    "    def forward(self, x):\n",
    "        # Pass the input through the first fully connected layer\n",
    "        x = self.fc1(x)\n",
    "        # Pass the output of the first fully connected layer through the first batch normalization layer\n",
    "        x = self.bn1(x)\n",
    "        # Pass the output of the first batch normalization layer through the first ReLU activation function\n",
    "        x = self.relu1(x)\n",
    "        \n",
    "        x = self.fc2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.relu2(x)\n",
    "        \n",
    "        x = self.fc3(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "net = NeuralNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Mutual Information\n",
    "\n",
    "'''\n",
    "#TODO Implement an function deriving the mutual information between X and Y according to Eq. (8) and (9). Implement the approximation of the mariginal distribution according to Eq.15.\n",
    "\n",
    "def shannon_entropy(probabilities: np.array) -> float:\n",
    "    \"\"\"\n",
    "    Calculate the Shannon entropy of a discrete random variable.\n",
    "    \n",
    "    Parameters:\n",
    "    probabilities (list or np.ndarray): The probabilities of each outcome of the random variable.\n",
    "    \n",
    "    Returns:\n",
    "    float: The Shannon entropy of the random variable.\n",
    "    \"\"\"\n",
    "\n",
    "    entropy = -np.sum(np.where(probabilities > 0, probabilities * np.log2(probabilities), 0))\n",
    "    \n",
    "    return entropy\n",
    "\n",
    "def mutual_information(probabilities: np.ndarray, conditionals: np.ndarray) -> float:\n",
    "    \"\"\"\n",
    "    Calculate the mutual information between two discrete random variables.\n",
    "    \n",
    "    Parameters:\n",
    "    probabilities (np.ndarray): The joint probabilities of the two random variables.\n",
    "    conditionals (np.ndarray): The conditional probabilities of each outcome of the second random variable given the first random variable.\n",
    "    \n",
    "    Returns:\n",
    "    float: The mutual information between the two random variables.\n",
    "    \"\"\"\n",
    "    marginal_entropy = shannon_entropy(np.sum(probabilities, axis=1))\n",
    "    conditional_entropy = np.sum(probabilities * shannon_entropy(conditionals), axis=1)\n",
    "    \n",
    "    return marginal_entropy - conditional_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nSelf-Augmented Training (SAT)\\n\\n'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Self-Augmented Training (SAT)\n",
    "\n",
    "'''\n",
    "#TODO Implement regularization penalty according to Eq. (4), (6), (6)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Training\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Evaluation Metric\n",
    "\n",
    "'''\n",
    "# TODO Implement the unsupervised clustering accuracy (ACC) according to Eq. (16)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "bd76f8a6c1bf20c479bc66ca0d5112f0d137b0d1ca4c89ae4addcd63596be2d9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

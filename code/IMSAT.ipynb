{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Libraries\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import math\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.metrics import adjusted_rand_score, normalized_mutual_info_score, silhouette_score\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.init as init\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import torchvision\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Setting the hyperparameters\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "num_epochs: int = 10\n",
    "batch_size: int = 32    # Should be set to a power of 2.\n",
    "# Learning rate\n",
    "lr:         float = 0.001\n",
    "# Trade-off parameter for mutual information and smooth regularization\n",
    "lam:        float = 0.1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Data Preprocessing\n",
    "\n",
    "\"\"\"\n",
    "#TODO Preprocess the AILARON dataset to a suitable format.\n",
    "\n",
    "# #TODO Implement custome dataset for AILARON data. Should inherit from torch.utils.data.Dataset\n",
    "# class AILARONDataset(torchvision.Dataset):\n",
    "\n",
    "#     def __init__(self):\n",
    "#         # Load data\n",
    "#         pass\n",
    "\n",
    "#     def __getitem__(self, index):\n",
    "#         # TODO\n",
    "#         pass\n",
    "#     def __len__(self):\n",
    "#         # TODO\n",
    "#         pass \n",
    "\n",
    "# ailaron_train = AILARONDataset()\n",
    "# dataloader = DataLoader(dataset=ailaron_train, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Load MNIST dataset, normalizes data and transform to tensor.\n",
    "mnist_train = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=torchvision.transforms.ToTensor())\n",
    "\n",
    "# # Get a random image from the dataset\n",
    "# image, label = mnist_train[np.random.randint(0, len(mnist_train))]\n",
    "\n",
    "# # Plot the image\n",
    "# plt.imshow(image[0], cmap='gray')\n",
    "# plt.title(f'Label: {label}')\n",
    "# plt.show()\n",
    "\n",
    "# Create DataLoader\n",
    "train_loader = DataLoader(mnist_train, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Conditional probability modeled as a Deep Neural Network\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        \n",
    "        # Add first fully connected layer with 28 * 28 = 784 input neurons and 1200 output neurons\n",
    "        self.fc1 = nn.Linear(28 * 28, 1200)\n",
    "        # Initialize the weights of the first fully connected layer using the He normal initialization\n",
    "        init.kaiming_normal_(self.fc1.weight, nonlinearity='relu')\n",
    "        # Add first batch normalization layer with 1200 neurons and epsilon = 2e-5\n",
    "        self.bn1   = nn.BatchNorm1d(1200, eps=2e-5)\n",
    "        self.bn1_F = nn.BatchNorm1d(1200, eps=2e-5, affine=False)\n",
    "        # Add first ReLU activation function\n",
    "        self.relu1 = nn.ReLU()\n",
    "        \n",
    "        self.fc2 = nn.Linear(1200, 1200)\n",
    "        init.kaiming_normal_(self.fc2.weight, nonlinearity='relu')\n",
    "        self.bn2   = nn.BatchNorm1d(1200, eps=2e-5)\n",
    "        self.bn2_F = nn.BatchNorm1d(1200, eps=2e-5, affine=False)\n",
    "\n",
    "        self.relu2 = nn.ReLU()\n",
    "        \n",
    "        # Add output layer of size 10 \n",
    "        self.fc3 = nn.Linear(1200, 10)\n",
    "        init.kaiming_normal_(self.fc3.weight, nonlinearity='linear')\n",
    "        \n",
    "    # Define the forward pass through the network\n",
    "    def forward(self, x):\n",
    "        # Pass the input through the first fully connected layer\n",
    "        x = self.fc1(x)\n",
    "        # Pass the output of the first fully connected layer through the first batch normalization layer\n",
    "        x = self.bn1(x)\n",
    "        # Pass the output of the first batch normalization layer through the first ReLU activation function\n",
    "        x = self.relu1(x)\n",
    "        \n",
    "        x = self.fc2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.relu2(x)\n",
    "        \n",
    "        x = self.fc3(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "net = NeuralNet()\n",
    "\n",
    "\"\"\"\n",
    "Approximation of the Marginal Distribution\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "def mariginal_distribution(conditionals: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Approximates the mariginal probability according to Eq (15)\n",
    "    \n",
    "    Args:\n",
    "    - conditionals: conditional probabilities\n",
    "\n",
    "    Returns\n",
    "    - An approximation of mariginal probabilities\n",
    "    \"\"\"\n",
    "\n",
    "    return torch.sum(conditionals) / conditionals.size()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Mutual Information\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "def shannon_entropy(probabilities: torch.Tensor) -> float:\n",
    "    \"\"\"\n",
    "    Computes the Shannon entropy of a tensor of probabilities. According to EEq. (9)\n",
    "    \n",
    "    Args:\n",
    "    - probabilities (torch.Tensor):\n",
    "    \n",
    "    Returns:\n",
    "    - float: the Shannon entropy\n",
    "    \"\"\"\n",
    "\n",
    "    return -torch.sum(probabilities * torch.log(probabilities))\n",
    "\n",
    "\n",
    "def mutual_information(probabilities: torch.Tensor, conditionals: torch.Tensor) -> float:\n",
    "    \"\"\"\n",
    "    Calculate the mutual information between two discrete random variables. According to Eq. (7)\n",
    "    \n",
    "    Args:\n",
    "    - probabilities (torch.Tensor): The joint probabilities of the two random variables.\n",
    "    - conditionals (torch.Tensor): The conditional probabilities of each outcome of the second random variable given the first random variable.\n",
    "    \n",
    "    Returns:\n",
    "    - float: The mutual information between the two random variables.\n",
    "    \"\"\"\n",
    "    marg_entropy = shannon_entropy(probabilities)\n",
    "    cond_entropy = shannon_entropy(conditionals)\n",
    "    \n",
    "    return marg_entropy - cond_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Self-Augmented Training (SAT)\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# def KL_divergence(p: torch.Tensor, q: torch.Tensor) -> float:\n",
    "#     \"\"\"\n",
    "#     Calculate the Kullback-Leibler divergence\n",
    "\n",
    "#     Args:\n",
    "#     - p (torch.Tensor): probability distribution\n",
    "#     - q (torch.Tensor): probability distribution\n",
    "\n",
    "#     Retruns:\n",
    "#     - the Kullback-Leibler divergence as a float\n",
    "#     \"\"\"\n",
    "\n",
    "#     return torch.sum(p * torch.log(p / q))\n",
    "\n",
    "def virtual_adversarial_perturbation(model: NeuralNet, x: torch.Tensor, eps: float = 1.0, ksi: float = 1e1, num_iters: int = 1) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Calculate the virtual adversarial perturbation for a batch of input samples x.\n",
    "\n",
    "    Args:\n",
    "        model: neural network model (unnormalized log probabilities of the model for the input samples)\n",
    "        x: input samples (batch_size x input_dim)\n",
    "        eps: perturbation size (float)\n",
    "        ksi: small constant used for computing the finite difference approximation of the KL divergence (float)\n",
    "        num_iters: number of iterations to use for computing the perturbation (int)\n",
    "\n",
    "    Returns:\n",
    "        d: virtual adversarial perturbation for the input samples (batch_size x input_dim)\n",
    "    \"\"\"\n",
    "\n",
    "    # Compute discrete representation of input samples.\n",
    "    y = F.softmax(model(x), dim=1)\n",
    "\n",
    "    # Initial perturbation with the same shape as input samples.\n",
    "    r = torch.randn_like(x)\n",
    "\n",
    "    for i in range(num_iters):\n",
    "        # Compute the discrete representation of the perturbed datapoints.\n",
    "        y_p = F.softmax(model(x + r * ksi), dim=1)\n",
    "     \n",
    "        # Compute the KL divergence between the probabilities\n",
    "        kl_div = F.kl_div(y.log(), y_p, reduction='batchmean')\n",
    "     \n",
    "        # Compute the gradient of current tensor w.r.t. graph leaves.\n",
    "        grad_r = torch.autograd.grad(kl_div, r, create_graph=True)[0]\n",
    "     \n",
    "        # Set the perturbation as the gradient of the KL divergence w.r.t. r\n",
    "        r = grad_r.detach()\n",
    "\n",
    "    return r * eps\n",
    "\n",
    "\n",
    "def virtual_adversarial_training(model: NeuralNet, x: torch.Tensor, eps: float = 1.0, xi: float = 1e-6, num_iters: int = 1):\n",
    "    \"\"\"Apply virtual adversarial training to a batch of input samples.\n",
    "\n",
    "    Args:\n",
    "        model: A neural network model.\n",
    "        x: Input samples of shape `(batch_size x input_dim)`.\n",
    "        eps: Perturbation size.\n",
    "        xi: A small constant used for computing the finite difference approximation of the KL divergence.\n",
    "        num_iters: The number of iterations to use for computing the perturbation.\n",
    "\n",
    "    Returns:\n",
    "        The total loss (sum of cross-entropy loss on original input and perturbed input) for the batch.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # Compute discrete representation of input samples.\n",
    "    y = F.softmax(model(x), dim=1)\n",
    "\n",
    "    # Compute the virtual adversarial perturbation for the input\n",
    "    vad = virtual_adversarial_perturbation(model, x, eps, xi, num_iters)\n",
    "\n",
    "    loss = F.kl_div(y.log(), model(x + vad), reduction='batchmean')\n",
    "\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1363284035.py, line 22)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn [58], line 22\u001b[1;36m\u001b[0m\n\u001b[1;33m    train_dataset = # Load the training dataset\u001b[0m\n\u001b[1;37m                    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Training the model\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Define the training function\n",
    "def train(model, train_loader, criterion, optimizer, num_epochs):\n",
    "    \"\"\"\n",
    "    Trains a given model using the provided training data, optimizer and loss criterion for a given number of epochs.\n",
    "\n",
    "    Args:\n",
    "        model: Neural network model to train.\n",
    "        train_loader: PyTorch data loader containing the training data.\n",
    "        criterion: Loss criterion used for training the model.\n",
    "        optimizer: Optimizer used to update the model's parameters.\n",
    "        num_epochs: Number of epochs to train the model.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # Loop over the epochs\n",
    "    for epoch in range(num_epochs):\n",
    "        # Initialize running loss for the epoch\n",
    "        running_loss = 0.0\n",
    "        # Loop over the mini-batches in the data loader\n",
    "        for i, data in enumerate(train_loader):\n",
    "            # Get the inputs and labels for the mini-batch\n",
    "            inputs, labels = data\n",
    "            # Zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "            # Forward pass through the model\n",
    "            outputs = model(inputs)\n",
    "            # Compute the loss\n",
    "            loss = criterion(outputs, labels)\n",
    "            # Backward pass through the model and compute gradients\n",
    "            loss.backward()\n",
    "            # Update the weights\n",
    "            optimizer.step()\n",
    "            # Accumulate the loss for the mini-batch\n",
    "            running_loss += loss.item()\n",
    "        # Compute the average loss for the epoch and print\n",
    "        print(f\"Epoch {epoch+1} loss: {running_loss/len(train_loader)}\")\n",
    "\n",
    "# Initialize the model, loss function, and optimizer\n",
    "model = NeuralNet()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr)\n",
    "\n",
    "# Train the model\n",
    "train(model, train_loader, criterion, optimizer, num_epochs)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# # For running net on single training example\n",
    "# net.eval()\n",
    "\n",
    "# cond_pr = F.softmax(net(test_image), dim=1)\n",
    "# marg_pr = cond_pr # TODO Implement according to Eq. 15\n",
    "\n",
    "# mutual_information(marg_pr, cond_pr)\n",
    "# # shannon_entropy(marg_pr)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Evaluation Metric\n",
    "\n",
    "\"\"\"\n",
    "# TODO Consider implementing the unsupervised clustering accuracy (ACC), see to Eq. (16).\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "bd76f8a6c1bf20c479bc66ca0d5112f0d137b0d1ca4c89ae4addcd63596be2d9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

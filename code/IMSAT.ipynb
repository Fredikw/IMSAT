{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.init as init\n",
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Data Preprocessing\n",
    "\n",
    "'''\n",
    "#TODO Preprocess the data to a suitable format\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision\n",
    "\n",
    "# Load MNIST dataset using torchvision\n",
    "mnist_dataset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=torchvision.transforms.ToTensor())\n",
    "\n",
    "# Get a random image from the dataset\n",
    "image, label = mnist_dataset[np.random.randint(0, len(mnist_dataset))]\n",
    "\n",
    "# # Plot the image\n",
    "# plt.imshow(image[0], cmap='gray')\n",
    "# plt.title(f'Label: {label}')\n",
    "# plt.show()\n",
    "\n",
    "test_image = image[0].view(-1, 28 * 28)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Settings\n",
    "\n",
    "'''\n",
    "#TODO Define the settings of the IMAST model.\n",
    "\n",
    "# Trade-off parameter for mutual information and smooth regularization\n",
    "lam = 0.1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Deep Neural Network\n",
    "\n",
    "'''\n",
    "\n",
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        \n",
    "        # Add first fully connected layer with 28 * 28 = 784 input neurons and 1200 output neurons\n",
    "        self.fc1 = nn.Linear(28 * 28, 1200)\n",
    "        # Initialize the weights of the first fully connected layer using the He normal initialization\n",
    "        init.kaiming_normal_(self.fc1.weight, nonlinearity='relu')\n",
    "        # Add first batch normalization layer with 1200 neurons and epsilon = 2e-5\n",
    "        self.bn1   = nn.BatchNorm1d(1200, eps=2e-5)\n",
    "        self.bn1_F = nn.BatchNorm1d(1200, eps=2e-5, affine=False)\n",
    "        # Add first ReLU activation function\n",
    "        self.relu1 = nn.ReLU()\n",
    "        \n",
    "        self.fc2 = nn.Linear(1200, 1200)\n",
    "        init.kaiming_normal_(self.fc2.weight, nonlinearity='relu')\n",
    "        self.bn2   = nn.BatchNorm1d(1200, eps=2e-5)\n",
    "        self.bn2_F = nn.BatchNorm1d(1200, eps=2e-5, affine=False)\n",
    "\n",
    "        self.relu2 = nn.ReLU()\n",
    "        \n",
    "        # Add output layer of size 10 \n",
    "        self.fc3 = nn.Linear(1200, 10)\n",
    "        init.kaiming_normal_(self.fc3.weight, nonlinearity='linear')\n",
    "        \n",
    "    # Define the forward pass through the network\n",
    "    def forward(self, x):\n",
    "        # Pass the input through the first fully connected layer\n",
    "        x = self.fc1(x)\n",
    "        # Pass the output of the first fully connected layer through the first batch normalization layer\n",
    "        x = self.bn1(x)\n",
    "        # Pass the output of the first batch normalization layer through the first ReLU activation function\n",
    "        x = self.relu1(x)\n",
    "        \n",
    "        x = self.fc2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.relu2(x)\n",
    "        \n",
    "        x = self.fc3(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "net = NeuralNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Mutual Information\n",
    "\n",
    "'''\n",
    "\n",
    "def shannon_entropy(probabilities: torch.Tensor) -> float:\n",
    "    \"\"\"\n",
    "    Computes the Shannon entropy of a tensor of probabilities. According to EEq. (9)\n",
    "    \n",
    "    Args:\n",
    "    - probabilities: a 1D PyTorch tensor of probabilities\n",
    "    \n",
    "    Returns:\n",
    "    - the Shannon entropy as a float\n",
    "    \"\"\"\n",
    "\n",
    "    return -torch.sum(probabilities * torch.log(probabilities))\n",
    "\n",
    "\n",
    "def mutual_information(probabilities: torch.Tensor, conditionals: torch.Tensor) -> float:\n",
    "    \"\"\"\n",
    "    Calculate the mutual information between two discrete random variables. According to Eq. (7)\n",
    "    \n",
    "    Parameters:\n",
    "    probabilities (torch.Tensor): The joint probabilities of the two random variables.\n",
    "    conditionals (torch.Tensor): The conditional probabilities of each outcome of the second random variable given the first random variable.\n",
    "    \n",
    "    Returns:\n",
    "    float: The mutual information between the two random variables.\n",
    "    \"\"\"\n",
    "    marg_entropy = shannon_entropy(probabilities)\n",
    "    cond_entropy = shannon_entropy(conditionals)\n",
    "    \n",
    "    return marg_entropy - cond_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nSelf-Augmented Training (SAT)\\n\\n'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Self-Augmented Training (SAT)\n",
    "\n",
    "'''\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def virtual_adversarial_perturbation(x, logit, eps=1.0, xi=1e-6, num_iters=1):\n",
    "    \"\"\"\n",
    "    Calculate the virtual adversarial perturbation for a batch of input samples x.\n",
    "    \n",
    "    Args:\n",
    "    - x: input samples (batch_size x input_dim)\n",
    "    - logit: unnormalized log probabilities of the model for the input samples (batch_size x num_classes)\n",
    "    - eps: perturbation size (float)\n",
    "    - xi: small constant used for computing the finite difference approximation of the KL divergence (float)\n",
    "    - num_iters: number of iterations to use for computing the perturbation (int)\n",
    "    \n",
    "    Returns:\n",
    "    - d: virtual adversarial perturbation for the input samples (batch_size x input_dim)\n",
    "    \"\"\"\n",
    "    # Sample random noise with the same shape as x\n",
    "    d = torch.randn_like(x)\n",
    "    for i in range(num_iters):\n",
    "        # Normalize the noise and scale it by xi\n",
    "        d = xi * F.normalize(d, dim=1)\n",
    "        # Make a copy of the logits and detach them from the computation graph\n",
    "        logit_p = logit.clone().detach()\n",
    "        logit_m = logit.clone().detach()\n",
    "        # Add the scaled noise to the logits\n",
    "        logit_p += eps * F.normalize(d, dim=1)\n",
    "        # Subtract the scaled noise from the logits\n",
    "        logit_m -= eps * F.normalize(d, dim=1)\n",
    "        # Compute the softmax probabilities of the perturbed logits\n",
    "        p_p = F.softmax(logit_p, dim=1)\n",
    "        p_m = F.softmax(logit_m, dim=1)\n",
    "        # Compute the KL divergence between the probabilities\n",
    "        kl = F.kl_div(p_p.log(), p_m, reduction='batchmean')\n",
    "        # Compute the gradient of the KL divergence w.r.t. the noise\n",
    "        d_grad, = torch.autograd.grad(kl, [d], retain_graph=False)\n",
    "        # Update the noise by taking a small step in the direction of the gradient\n",
    "        d = d_grad.detach()\n",
    "    # Normalize the final noise and scale it by xi\n",
    "    return xi * F.normalize(d, dim=1)\n",
    "\n",
    "def virtual_adversarial_training(model, x, y, eps=1.0, xi=1e-6, num_iters=1):\n",
    "    \"\"\"\n",
    "    Apply virtual adversarial training to a batch of input samples x and labels y.\n",
    "    \n",
    "    Args:\n",
    "    - model: neural network model\n",
    "    - x: input samples (batch_size x input_dim)\n",
    "    - y: labels for the input samples (batch_size)\n",
    "    - eps: perturbation size (float)\n",
    "    - xi: small constant used for computing the finite difference approximation of the KL divergence (float)\n",
    "    - num_iters: number of iterations to use for computing the perturbation (int)\n",
    "    \n",
    "    Returns:\n",
    "    - loss: total loss (sum of cross-entropy loss on original input and perturbed input) for the batch (float)\n",
    "    \"\"\"\n",
    "    # Compute the logits of the model on the input\n",
    "    logit = model(x)\n",
    "    # Detach the logits from the computation graph\n",
    "    logit.detach_()\n",
    "    # Compute the virtual adversarial perturbation for the input\n",
    "    vadv = virtual_adversarial_perturbation(x, logit, eps, xi, num_iters)\n",
    "    # Compute the logits of the model on the perturbed input\n",
    "    logit_p = model(x + vadv)\n",
    "    # Compute the cross-entropy loss on the original input\n",
    "    loss = F.cross_entropy\n",
    "\n",
    "\n",
    "#TODO Implement Virtual Adversarial Training (VAT)\n",
    "\n",
    "#TODO Implement regularization penalty according to Eq. (4), (6), (6)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0., grad_fn=<SubBackward0>)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "\n",
    "'''\n",
    "\n",
    "# # For running net on single training example\n",
    "# net.eval()\n",
    "\n",
    "# cond_pr = F.softmax(net(test_image), dim=1)\n",
    "# marg_pr = cond_pr # TODO Implement according to Eq. 15\n",
    "\n",
    "# mutual_information(marg_pr, cond_pr)\n",
    "# # shannon_entropy(marg_pr)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Training\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Evaluation Metric\n",
    "\n",
    "'''\n",
    "# TODO Implement the unsupervised clustering accuracy (ACC) according to Eq. (16)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "bd76f8a6c1bf20c479bc66ca0d5112f0d137b0d1ca4c89ae4addcd63596be2d9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

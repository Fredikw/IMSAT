{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Libraries\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import csv\n",
    "from datetime import datetime\n",
    "from typing import Callable\n",
    "\n",
    "import logging\n",
    "\n",
    "import numpy as np\n",
    "import csv\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "from sklearn.metrics import accuracy_score, balanced_accuracy_score, confusion_matrix\n",
    "from sklearn.metrics.cluster import normalized_mutual_info_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of available devices: 1\n",
      " Device name: NVIDIA A100 80GB PCIe\n",
      " Total GPU memory device 0: 79.20 GB\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Setting generic hyperparameters\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "num_epochs: int = 80\n",
    "batch_size: int = 256 # Should be set to a power of 2.\n",
    "# Learning rate\n",
    "lr:         float = 1e-4 # Learning rate used in the IIC paper: lr=1e-4.\n",
    "\n",
    "\"\"\"\n",
    "GPU utilization\n",
    "\n",
    "\"\"\"\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Specifications\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"Number of available devices: {torch.cuda.device_count()}\\n\",\n",
    "          f\"Device name: {torch.cuda.get_device_name(torch.cuda.current_device())}\\n\",\n",
    "          f\"Total GPU memory device 0: {torch.cuda.get_device_properties(0).total_memory/(1024**3):.2f} GB\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "44"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Store data to .csv file\n",
    "\n",
    "'''\n",
    "\n",
    "# open the file for writing\n",
    "f = open(f'logs/IIC_ten_classes_balanced/{datetime.now().strftime(\"%Y-%m-%d-%H-%M\")}.csv', 'w')\n",
    "# create a CSV writer object\n",
    "writer = csv.writer(f)\n",
    "# write the header row to the CSV file\n",
    "writer.writerow(['epoch', 'loss', 'running_acc', 'acc', 'running_nmi', 'nmi'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Unsupervised Machine Learning Framework\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "def train(model, data_loader: DataLoader, criterion: Callable, optimizer: torch.optim, num_epochs: int, num_classes: int=None) -> None:\n",
    "    \"\"\"\n",
    "    Trains a given model using the provided training data, optimizer and loss criterion for a given number of epochs.\n",
    "\n",
    "    Args:\n",
    "        model: Neural network model to train.\n",
    "        data_loader: PyTorch data loader containing the training data.\n",
    "        criterion: Loss criterion used for training the model.\n",
    "        optimizer: Optimizer used to update the model's parameters.\n",
    "        num_epochs: Number of epochs to train the model.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "\n",
    "        running_loss = 0.0\n",
    "        running_acc  = 0.0\n",
    "        running_nmi  = 0.0\n",
    "\n",
    "        # Initialize tensors for storing true and predicted labels\n",
    "        labels_true = torch.zeros(len(data_loader.dataset))\n",
    "        labels_pred = torch.zeros(len(data_loader.dataset))\n",
    "\n",
    "        # Loop over the mini-batches in the data loader\n",
    "        for i, data in enumerate(data_loader):\n",
    "        \n",
    "            # Get the inputs and labels for the mini-batch\n",
    "            inputs, labels = data\n",
    "\n",
    "            # Use GPU if available\n",
    "            inputs = inputs.to(device)\n",
    "\n",
    "            # Image augmentation\n",
    "            if data_loader.dataset.augment_data:\n",
    "                inputs_trans = torch.stack([data_loader.dataset.transform_list(input) for input in inputs])\n",
    "                # # Flatten input data for the feed forward model\n",
    "                # inputs       = [inputs.view(inputs.size(0), -1), inputs_trans.view(inputs_trans.size(0), -1)]\n",
    "                inputs       = [inputs, inputs_trans]\n",
    "            # else:\n",
    "                # inputs = inputs.view(inputs.size(0), -1)\n",
    "        \n",
    "            # Zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass through the model\n",
    "            if data_loader.dataset.augment_data:\n",
    "                outputs = [F.softmax(model(inputs[0]), dim=1), F.softmax(model(inputs[1]), dim=1)]\n",
    "            else:\n",
    "                outputs = F.softmax(model(inputs), dim=1)\n",
    "\n",
    "            # Set arguments for objective function\n",
    "            # kwargs = {key: value for key, value in locals().items() if key in criterion.__code__.co_varnames}\n",
    "            kwargs = {\"model\": model, \"inputs\": inputs, \"outputs\": outputs}\n",
    "            kwargs = {key: value for key, value in kwargs.items() if key in criterion.__code__.co_varnames}\n",
    "            \n",
    "            # Compute the loss\n",
    "            loss = criterion(**kwargs)\n",
    "            # Backward pass through the model and compute gradients\n",
    "            loss.backward()\n",
    "        \n",
    "            # Update the weights\n",
    "            optimizer.step()\n",
    "\n",
    "            # Accumulate the loss for the mini-batch\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            outputs = outputs[0] if data_loader.dataset.augment_data else outputs\n",
    "\n",
    "            running_acc  += unsupervised_clustering_accuracy(labels, torch.argmax(outputs.cpu(), dim=1),C=num_classes)\n",
    "            running_nmi  += normalized_mutual_info_score(labels, torch.argmax(outputs.cpu(), dim=1))\n",
    "\n",
    "            # Store predicted and true labels in tensors\n",
    "            labels_true[i*len(labels):(i+1)*len(labels)] = labels\n",
    "            labels_pred[i*len(labels):(i+1)*len(labels)] = torch.argmax(outputs, dim=1)\n",
    "\n",
    "        acc = unsupervised_clustering_accuracy(labels_true, labels_pred, C=num_classes)\n",
    "        nmi = normalized_mutual_info_score(labels_true, labels_pred)\n",
    "\n",
    "        # Compute the average loss and accuracy for the epoch and print\n",
    "        print(f\"Epoch {epoch+1} loss: {running_loss/len(data_loader):.4f},\\\n",
    "              running_acc: {running_acc/len(data_loader):.4f}, acc: {acc:.4f},\\\n",
    "              running_nmi: {running_nmi/len(data_loader):.4f}, nmi: {nmi:.4f}\")\n",
    "        # Store data to file\n",
    "        writer.writerow([epoch+1, running_loss/len(data_loader), running_acc/len(data_loader), acc, running_nmi/len(data_loader), nmi])\n",
    "\n",
    "def reassign(y_true: torch.Tensor, y_pred: torch.Tensor, C: int=None) -> float:\n",
    "    \n",
    "    # Create confusion matrix\n",
    "    cm = confusion_matrix(y_pred, y_true, labels=list(range(C)))\n",
    "\n",
    "    # Compute best matching between true and predicted labels using the Hungarian algorithm\n",
    "    _, col_ind = linear_sum_assignment(-cm)\n",
    "\n",
    "    # Reassign labels for the predicted clusters\n",
    "    y_pred_reassigned = torch.tensor(col_ind)[y_pred.long()]\n",
    "    \n",
    "    return y_pred_reassigned\n",
    "        \n",
    "def unsupervised_clustering_accuracy(y_true: torch.Tensor, y_pred: torch.Tensor, C: int=None) -> float:\n",
    "    \"\"\"\n",
    "    Computes the unsupervised clustering accuracy between two clusterings.\n",
    "    Uses the Hungarian algorithm to find the best matching between true and predicted labels.\n",
    "\n",
    "    Args:\n",
    "        y_true: true cluster labels as a 1D torch.Tensor\n",
    "        y_pred: predicted cluster labels as a 1D torch.Tensor\n",
    "        C:      number of classes\n",
    "\n",
    "    Returns:\n",
    "        accuracy: unsupervised clustering accuracy as a float\n",
    "    \"\"\"\n",
    "    \n",
    "    y_pred_reassigned = reassign(y_true, y_pred, C)\n",
    "\n",
    "    # Compute accuracy as the percentage of correctly classified samples\n",
    "    acc = accuracy_score(y_true, y_pred_reassigned)\n",
    "\n",
    "    return acc\n",
    "\n",
    "def unsupervised_balanced_clustering_accuracy(y_true: torch.Tensor, y_pred: torch.Tensor, C: int=None) -> float:\n",
    "    \"\"\"\n",
    "    Computes the unsupervised clustering accuracy between two clusterings.\n",
    "    Uses the Hungarian algorithm to find the best matching between true and predicted labels.\n",
    "\n",
    "    Args:\n",
    "        y_true: true cluster labels as a 1D torch.Tensor\n",
    "        y_pred: predicted cluster labels as a 1D torch.Tensor\n",
    "        C:      number of classes\n",
    "\n",
    "    Returns:\n",
    "        accuracy: unsupervised clustering accuracy as a float\n",
    "    \"\"\"\n",
    "    \n",
    "    y_pred_reassigned = reassign(y_true, y_pred, C)\n",
    "\n",
    "    # Compute accuracy as the percentage of correctly classified samples\n",
    "    acc = balanced_accuracy_score(y_true, y_pred_reassigned)\n",
    "\n",
    "    return acc\n",
    "\n",
    "\n",
    "def test_classifier(model, data_loader: DataLoader, num_classes: int) -> float:\n",
    "    \"\"\"\n",
    "    Testing a classifier given the model and a test set.\n",
    "\n",
    "    Args:\n",
    "        model: Neural network model to train.\n",
    "        data_loader: PyTorch data loader containing the test data.\n",
    "    \n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    \n",
    "    # Disable gradient computation, not needed for inference\n",
    "    model.eval()\n",
    "    # Initialize tensors for storing true and predicted labels\n",
    "    y_true = torch.zeros(len(data_loader.dataset))\n",
    "    y_pred = torch.zeros(len(data_loader.dataset))\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Iterate over the mini-batches in the data loader\n",
    "        for i, data in enumerate(data_loader):\n",
    "            # Get the inputs and true labels for the mini-batch and reshape\n",
    "            inputs, labels_true = data\n",
    "            \n",
    "            # Use GPU if available\n",
    "            inputs      = inputs.to(device)\n",
    "                                    \n",
    "            # # TODO flattening should be done in the feed forward model, else statement should be removed\n",
    "            # inputs = inputs.view(inputs.size(0), -1)\n",
    "            \n",
    "            # Forward pass through the model to get predicted labels\n",
    "            labels_pred = F.softmax(model(inputs), dim=1)\n",
    "\n",
    "            # Store predicted and true labels in tensors\n",
    "            y_pred[i*len(labels_true):(i+1)*len(labels_true)] = torch.argmax(labels_pred.cpu(), dim=1)\n",
    "            y_true[i*len(labels_true):(i+1)*len(labels_true)] = labels_true\n",
    "\n",
    "    # Compute unsupervised clustering accuracy score\n",
    "    acc = unsupervised_clustering_accuracy(y_true, y_pred, C=num_classes)\n",
    "    \n",
    "    acc_balanced = unsupervised_balanced_clustering_accuracy(y_true, y_pred, C=num_classes)\n",
    "    \n",
    "    y_reassign = reassign(y_true, y_pred, C=num_classes)\n",
    "\n",
    "    # Let's assume y_true and y_pred are your tensors\n",
    "    y_true = y_true.tolist()  # Convert tensors to lists\n",
    "    y_pred = y_reassign.tolist()\n",
    "\n",
    "    # Zip the two lists together\n",
    "    data = list(zip(y_true, y_pred))\n",
    "\n",
    "    # Write the data to a CSV file\n",
    "    with open('output.csv', 'w', newline='') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow([\"y_true\", \"y_pred\"])  # Write the header\n",
    "        writer.writerows(data)  # Write the data rows\n",
    "\n",
    "\n",
    "    print(f\"\\nThe unsupervised clustering accuracy score of the classifier is: {acc}\")\n",
    "    \n",
    "    return acc, acc_balanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "from archt import get_model\n",
    "\n",
    "# Information Maximizing Self-Augmented Training\n",
    "from IMSAT import regularized_information_maximization\n",
    "\n",
    "# Invariant Information Clustering\n",
    "from IIC import invariant_information_clustering\n",
    "\n",
    "from datasets.dataset_classes import NDSBDataset, MNISTDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Create the train and test datasets\n",
    "train_dataset = NDSBDataset(train=True, augment_data=True)\n",
    "test_dataset  = NDSBDataset(train=False)\n",
    "\n",
    "# Create the train and test data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader  = DataLoader(test_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model specifications: ResNet(\n",
      "  (conv1): Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU(inplace=True)\n",
      "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  (layer1): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (fc): Linear(in_features=512, out_features=10, bias=True)\n",
      ")\n",
      "Epoch 1 loss: -0.0570,              running_acc: 0.3300, acc: 0.3054,              running_nmi: 0.2776, nmi: 0.2079\n",
      "Epoch 2 loss: -0.2638,              running_acc: 0.3386, acc: 0.3212,              running_nmi: 0.3303, nmi: 0.3178\n",
      "Epoch 3 loss: -0.4029,              running_acc: 0.3231, acc: 0.3192,              running_nmi: 0.3355, nmi: 0.3256\n",
      "Epoch 4 loss: -0.5279,              running_acc: 0.3260, acc: 0.3320,              running_nmi: 0.3692, nmi: 0.3657\n",
      "Epoch 5 loss: -0.6096,              running_acc: 0.3744, acc: 0.3685,              running_nmi: 0.4145, nmi: 0.4116\n",
      "Epoch 6 loss: -0.6752,              running_acc: 0.4080, acc: 0.3931,              running_nmi: 0.4443, nmi: 0.4315\n",
      "Epoch 7 loss: -0.7358,              running_acc: 0.4131, acc: 0.4089,              running_nmi: 0.4579, nmi: 0.4457\n",
      "Epoch 8 loss: -0.7774,              running_acc: 0.4247, acc: 0.4197,              running_nmi: 0.4837, nmi: 0.4725\n",
      "Epoch 9 loss: -0.8208,              running_acc: 0.4266, acc: 0.4315,              running_nmi: 0.4979, nmi: 0.4810\n",
      "Epoch 10 loss: -0.8294,              running_acc: 0.4325, acc: 0.4296,              running_nmi: 0.5012, nmi: 0.4838\n",
      "Epoch 11 loss: -0.8562,              running_acc: 0.4246, acc: 0.4187,              running_nmi: 0.5042, nmi: 0.4913\n",
      "Epoch 12 loss: -0.8836,              running_acc: 0.4342, acc: 0.4217,              running_nmi: 0.5248, nmi: 0.5103\n",
      "Epoch 13 loss: -0.8879,              running_acc: 0.4494, acc: 0.4256,              running_nmi: 0.5256, nmi: 0.5084\n",
      "Epoch 14 loss: -0.8917,              running_acc: 0.4283, acc: 0.4325,              running_nmi: 0.5363, nmi: 0.5177\n",
      "Epoch 15 loss: -0.8855,              running_acc: 0.4375, acc: 0.4443,              running_nmi: 0.5301, nmi: 0.5215\n",
      "Epoch 16 loss: -0.9088,              running_acc: 0.4539, acc: 0.4453,              running_nmi: 0.5314, nmi: 0.5209\n",
      "Epoch 17 loss: -0.9203,              running_acc: 0.4479, acc: 0.4483,              running_nmi: 0.5328, nmi: 0.5220\n",
      "Epoch 18 loss: -0.9357,              running_acc: 0.4582, acc: 0.4522,              running_nmi: 0.5219, nmi: 0.5094\n",
      "Epoch 19 loss: -0.9383,              running_acc: 0.4502, acc: 0.4394,              running_nmi: 0.5154, nmi: 0.4983\n",
      "Epoch 20 loss: -0.9651,              running_acc: 0.4711, acc: 0.4798,              running_nmi: 0.5266, nmi: 0.5095\n",
      "Epoch 21 loss: -0.9970,              running_acc: 0.4884, acc: 0.4936,              running_nmi: 0.5319, nmi: 0.5142\n",
      "Epoch 22 loss: -1.0446,              running_acc: 0.4873, acc: 0.4877,              running_nmi: 0.5382, nmi: 0.5196\n",
      "Epoch 23 loss: -1.0603,              running_acc: 0.5201, acc: 0.5133,              running_nmi: 0.5636, nmi: 0.5397\n",
      "Epoch 24 loss: -1.0767,              running_acc: 0.5380, acc: 0.5429,              running_nmi: 0.5853, nmi: 0.5685\n",
      "Epoch 25 loss: -1.1147,              running_acc: 0.5480, acc: 0.5448,              running_nmi: 0.6036, nmi: 0.5848\n",
      "Epoch 26 loss: -1.1541,              running_acc: 0.5508, acc: 0.5488,              running_nmi: 0.6094, nmi: 0.5875\n",
      "Epoch 27 loss: -1.1184,              running_acc: 0.5400, acc: 0.5389,              running_nmi: 0.6011, nmi: 0.5768\n",
      "Epoch 28 loss: -1.1593,              running_acc: 0.5607, acc: 0.5537,              running_nmi: 0.6237, nmi: 0.6028\n",
      "Epoch 29 loss: -1.1113,              running_acc: 0.5437, acc: 0.5527,              running_nmi: 0.6127, nmi: 0.6004\n",
      "Epoch 30 loss: -1.1367,              running_acc: 0.5562, acc: 0.5547,              running_nmi: 0.6273, nmi: 0.6057\n",
      "Epoch 31 loss: -1.1431,              running_acc: 0.5596, acc: 0.5567,              running_nmi: 0.6223, nmi: 0.5989\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32 loss: -1.1496,              running_acc: 0.5626, acc: 0.5626,              running_nmi: 0.6283, nmi: 0.6022\n",
      "Epoch 33 loss: -1.1742,              running_acc: 0.5707, acc: 0.5655,              running_nmi: 0.6299, nmi: 0.6158\n",
      "Epoch 34 loss: -1.1583,              running_acc: 0.5620, acc: 0.5635,              running_nmi: 0.6396, nmi: 0.6140\n",
      "Epoch 35 loss: -1.1517,              running_acc: 0.5609, acc: 0.5645,              running_nmi: 0.6332, nmi: 0.6112\n",
      "Epoch 36 loss: -1.1912,              running_acc: 0.5557, acc: 0.5576,              running_nmi: 0.6228, nmi: 0.6054\n",
      "Epoch 37 loss: -1.1628,              running_acc: 0.5595, acc: 0.5576,              running_nmi: 0.6336, nmi: 0.6102\n",
      "Epoch 38 loss: -1.2162,              running_acc: 0.5682, acc: 0.5616,              running_nmi: 0.6332, nmi: 0.6134\n",
      "Epoch 39 loss: -1.1929,              running_acc: 0.5715, acc: 0.5626,              running_nmi: 0.6355, nmi: 0.6128\n",
      "Epoch 40 loss: -1.2082,              running_acc: 0.5685, acc: 0.5645,              running_nmi: 0.6386, nmi: 0.6166\n",
      "Epoch 41 loss: -1.2164,              running_acc: 0.5635, acc: 0.5665,              running_nmi: 0.6302, nmi: 0.6067\n",
      "Epoch 42 loss: -1.2120,              running_acc: 0.5612, acc: 0.5596,              running_nmi: 0.6293, nmi: 0.6009\n",
      "Epoch 43 loss: -1.2222,              running_acc: 0.5768, acc: 0.5773,              running_nmi: 0.6344, nmi: 0.6104\n",
      "Epoch 44 loss: -1.2583,              running_acc: 0.5657, acc: 0.5724,              running_nmi: 0.6239, nmi: 0.6012\n",
      "Epoch 45 loss: -1.2673,              running_acc: 0.5815, acc: 0.5852,              running_nmi: 0.6319, nmi: 0.6048\n",
      "Epoch 46 loss: -1.2877,              running_acc: 0.5753, acc: 0.5803,              running_nmi: 0.6223, nmi: 0.6036\n",
      "Epoch 47 loss: -1.2828,              running_acc: 0.5772, acc: 0.5724,              running_nmi: 0.6228, nmi: 0.5957\n",
      "Epoch 48 loss: -1.2946,              running_acc: 0.5813, acc: 0.5833,              running_nmi: 0.6346, nmi: 0.6128\n",
      "Epoch 49 loss: -1.2693,              running_acc: 0.5780, acc: 0.5783,              running_nmi: 0.6375, nmi: 0.6189\n",
      "Epoch 50 loss: -1.2853,              running_acc: 0.5733, acc: 0.5764,              running_nmi: 0.6307, nmi: 0.6095\n",
      "Epoch 51 loss: -1.2678,              running_acc: 0.5695, acc: 0.5685,              running_nmi: 0.6273, nmi: 0.6008\n",
      "Epoch 52 loss: -1.2913,              running_acc: 0.5673, acc: 0.5724,              running_nmi: 0.6267, nmi: 0.6065\n",
      "Epoch 53 loss: -1.2818,              running_acc: 0.5870, acc: 0.5842,              running_nmi: 0.6365, nmi: 0.6093\n",
      "Epoch 54 loss: -1.2745,              running_acc: 0.5833, acc: 0.5833,              running_nmi: 0.6348, nmi: 0.6179\n",
      "Epoch 55 loss: -1.3375,              running_acc: 0.5772, acc: 0.5754,              running_nmi: 0.6309, nmi: 0.6088\n",
      "Epoch 56 loss: -1.3201,              running_acc: 0.5946, acc: 0.5862,              running_nmi: 0.6305, nmi: 0.6104\n",
      "Epoch 57 loss: -1.3187,              running_acc: 0.5929, acc: 0.5901,              running_nmi: 0.6271, nmi: 0.6097\n",
      "Epoch 58 loss: -1.3219,              running_acc: 0.5801, acc: 0.5892,              running_nmi: 0.6288, nmi: 0.6102\n",
      "Epoch 59 loss: -1.3335,              running_acc: 0.5723, acc: 0.5724,              running_nmi: 0.6259, nmi: 0.6061\n",
      "Epoch 60 loss: -1.3245,              running_acc: 0.5767, acc: 0.5734,              running_nmi: 0.6322, nmi: 0.6115\n",
      "Epoch 61 loss: -1.3130,              running_acc: 0.5924, acc: 0.5803,              running_nmi: 0.6394, nmi: 0.6091\n",
      "Epoch 62 loss: -1.3014,              running_acc: 0.5910, acc: 0.5872,              running_nmi: 0.6365, nmi: 0.6104\n",
      "Epoch 63 loss: -1.3574,              running_acc: 0.5823, acc: 0.5783,              running_nmi: 0.6419, nmi: 0.6200\n",
      "Epoch 64 loss: -1.3374,              running_acc: 0.5885, acc: 0.5793,              running_nmi: 0.6337, nmi: 0.6123\n",
      "Epoch 65 loss: -1.3442,              running_acc: 0.5889, acc: 0.5823,              running_nmi: 0.6304, nmi: 0.6109\n",
      "Epoch 66 loss: -1.3473,              running_acc: 0.5775, acc: 0.5872,              running_nmi: 0.6392, nmi: 0.6139\n",
      "Epoch 67 loss: -1.3083,              running_acc: 0.5941, acc: 0.5862,              running_nmi: 0.6293, nmi: 0.6059\n",
      "Epoch 68 loss: -1.3131,              running_acc: 0.5952, acc: 0.5951,              running_nmi: 0.6442, nmi: 0.6226\n",
      "Epoch 69 loss: -1.3269,              running_acc: 0.5971, acc: 0.5901,              running_nmi: 0.6306, nmi: 0.6117\n",
      "Epoch 70 loss: -1.3353,              running_acc: 0.5775, acc: 0.5872,              running_nmi: 0.6302, nmi: 0.6131\n",
      "Epoch 71 loss: -1.3137,              running_acc: 0.5911, acc: 0.5941,              running_nmi: 0.6332, nmi: 0.6079\n",
      "Epoch 72 loss: -1.3111,              running_acc: 0.5853, acc: 0.5921,              running_nmi: 0.6285, nmi: 0.6164\n",
      "Epoch 73 loss: -1.2357,              running_acc: 0.5849, acc: 0.5961,              running_nmi: 0.6285, nmi: 0.6083\n",
      "Epoch 74 loss: -1.2890,              running_acc: 0.5872, acc: 0.5862,              running_nmi: 0.6224, nmi: 0.5994\n",
      "Epoch 75 loss: -1.3249,              running_acc: 0.6041, acc: 0.6039,              running_nmi: 0.6488, nmi: 0.6208\n",
      "Epoch 76 loss: -1.2900,              running_acc: 0.6068, acc: 0.6030,              running_nmi: 0.6440, nmi: 0.6192\n",
      "Epoch 77 loss: -1.3111,              running_acc: 0.5869, acc: 0.5911,              running_nmi: 0.6323, nmi: 0.6131\n",
      "Epoch 78 loss: -1.3410,              running_acc: 0.5931, acc: 0.5951,              running_nmi: 0.6352, nmi: 0.6168\n",
      "Epoch 79 loss: -1.3169,              running_acc: 0.5864, acc: 0.5842,              running_nmi: 0.6181, nmi: 0.5995\n",
      "Epoch 80 loss: -1.3576,              running_acc: 0.5866, acc: 0.5833,              running_nmi: 0.6280, nmi: 0.6069\n",
      "\n",
      "The unsupervised clustering accuracy score of the classifier is: 0.7050147492625368\n"
     ]
    }
   ],
   "source": [
    "# Initialize model\n",
    "model = get_model(\"resnet18\", num_classes=10).to(device)\n",
    "\n",
    "# Initialize loss function, and optimizer\n",
    "criterion = invariant_information_clustering\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "# Store metadata to .log file\n",
    "logger = logging.getLogger(__name__)\n",
    "# Set the logging level\n",
    "logger.setLevel(logging.INFO)\n",
    "# Add handler to the logger\n",
    "logger.addHandler(logging.FileHandler(f'logs/IIC_ten_classes_balanced/{datetime.now().strftime(\"%Y-%m-%d-%H-%M\")}.log'))\n",
    "\n",
    "# Write metadata to .log file\n",
    "logger.info(f'Optimization criterion: {criterion.__name__}')\n",
    "logger.info(f'Learning rate: {lr}')\n",
    "logger.info(f'Number of epochs: {num_epochs}')\n",
    "logger.info(f'Batch size: {batch_size}')\n",
    "logger.info(f'Optimizer: {optimizer}')\n",
    "logger.info(f'Model: {model}')\n",
    "\n",
    "# Train the model\n",
    "train(model, train_loader, criterion, optimizer, num_epochs, num_classes=10)\n",
    "\n",
    "# Test model\n",
    "acc, acc_balanced = test_classifier(model, test_loader,num_classes=10)\n",
    "\n",
    "logger.info(f'Accuracy: {acc}')\n",
    "logger.info(f'Balanced Accuracy: {acc_balanced}')\n",
    "# Close data file\n",
    "f.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "bd76f8a6c1bf20c479bc66ca0d5112f0d137b0d1ca4c89ae4addcd63596be2d9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Libraries\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import csv\n",
    "from datetime import datetime\n",
    "from typing import Callable\n",
    "\n",
    "import logging\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.metrics.cluster import normalized_mutual_info_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of available devices: 2\n",
      " Device name: NVIDIA GeForce RTX 2080 Ti\n",
      " Total GPU memory device 0: 10.76 GB\n",
      " Total GPU memory device 1: 10.76 GB\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Setting generic hyperparameters\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "num_epochs: int = 5#20\n",
    "batch_size: int = 256    # Should be set to a power of 2.\n",
    "# Learning rate\n",
    "lr:         float = 1e-4 # Learning rate used in the IIC paper: lr=1e-4.\n",
    "\n",
    "\"\"\"\n",
    "GPU utilization\n",
    "\n",
    "\"\"\"\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Specifications\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"Number of available devices: {torch.cuda.device_count()}\\n\",\n",
    "          f\"Device name: {torch.cuda.get_device_name(torch.cuda.current_device())}\\n\",\n",
    "          f\"Total GPU memory device 0: {torch.cuda.get_device_properties(0).total_memory/(1024**3):.2f} GB\\n\",\n",
    "          f\"Total GPU memory device 1: {torch.cuda.get_device_properties(1).total_memory/(1024**3):.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Store data to .csv file\n",
    "\n",
    "'''\n",
    "\n",
    "# open the file for writing\n",
    "f = open(f'logs/{datetime.now().strftime(\"%Y-%m-%d-%H-%M\")}.csv', 'w')\n",
    "# create a CSV writer object\n",
    "writer = csv.writer(f)\n",
    "# write the header row to the CSV file\n",
    "writer.writerow(['epoch', 'loss', 'running_acc', 'acc', 'running_nmi', 'nmi'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Unsupervised Machine Learning Framework\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "def train(model, data_loader: DataLoader, criterion: Callable, optimizer: torch.optim, num_epochs: int) -> None:\n",
    "    \"\"\"\n",
    "    Trains a given model using the provided training data, optimizer and loss criterion for a given number of epochs.\n",
    "\n",
    "    Args:\n",
    "        model: Neural network model to train.\n",
    "        data_loader: PyTorch data loader containing the training data.\n",
    "        criterion: Loss criterion used for training the model.\n",
    "        optimizer: Optimizer used to update the model's parameters.\n",
    "        num_epochs: Number of epochs to train the model.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "\n",
    "        running_loss = 0.0\n",
    "        running_acc  = 0.0\n",
    "        running_nmi  = 0.0\n",
    "\n",
    "        # Initialize tensors for storing true and predicted labels\n",
    "        labels_true = torch.zeros(len(data_loader.dataset))\n",
    "        labels_pred = torch.zeros(len(data_loader.dataset))\n",
    "\n",
    "        # Loop over the mini-batches in the data loader\n",
    "        for i, data in enumerate(data_loader):\n",
    "        \n",
    "            # Get the inputs and labels for the mini-batch\n",
    "            inputs, labels = data\n",
    "\n",
    "            # Use GPU if available\n",
    "            inputs = inputs.to(device)\n",
    "\n",
    "            # Image augmentation\n",
    "            if data_loader.dataset.augment_data:\n",
    "                inputs_trans = data_loader.dataset.transform_list(inputs)\n",
    "                # # Flatten input data for the feed forward model\n",
    "                # inputs       = [inputs.view(inputs.size(0), -1), inputs_trans.view(inputs_trans.size(0), -1)]\n",
    "                inputs       = [inputs, inputs_trans]\n",
    "            # else:\n",
    "                # inputs = inputs.view(inputs.size(0), -1)\n",
    "        \n",
    "            # Zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass through the model\n",
    "            if data_loader.dataset.augment_data:\n",
    "                outputs = [F.softmax(model(inputs[0]), dim=1), F.softmax(model(inputs[1]), dim=1)]\n",
    "            else:\n",
    "                outputs = F.softmax(model(inputs), dim=1)\n",
    "\n",
    "            # Set arguments for objective function\n",
    "            # kwargs = {key: value for key, value in locals().items() if key in criterion.__code__.co_varnames}\n",
    "            kwargs = {\"model\": model, \"inputs\": inputs, \"outputs\": outputs}\n",
    "            kwargs = {key: value for key, value in kwargs.items() if key in criterion.__code__.co_varnames}\n",
    "            \n",
    "            # Compute the loss\n",
    "            loss = criterion(**kwargs)\n",
    "            # Backward pass through the model and compute gradients\n",
    "            loss.backward()\n",
    "        \n",
    "            # Update the weights\n",
    "            optimizer.step()\n",
    "\n",
    "            # Accumulate the loss for the mini-batch\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            outputs = outputs[0] if data_loader.dataset.augment_data else outputs\n",
    "\n",
    "            running_acc  += unsupervised_clustering_accuracy(labels, torch.argmax(outputs.cpu(), dim=1))\n",
    "            running_nmi  += normalized_mutual_info_score(labels, torch.argmax(outputs.cpu(), dim=1))\n",
    "\n",
    "            # Store predicted and true labels in tensors\n",
    "            labels_true[i*len(labels):(i+1)*len(labels)] = labels\n",
    "            labels_pred[i*len(labels):(i+1)*len(labels)] = torch.argmax(outputs, dim=1)\n",
    "\n",
    "        acc = unsupervised_clustering_accuracy(labels_true, labels_pred)\n",
    "        nmi = normalized_mutual_info_score(labels_true, labels_pred)\n",
    "\n",
    "        # Compute the average loss and accuracy for the epoch and print\n",
    "        print(f\"Epoch {epoch+1} loss: {running_loss/len(data_loader):.4f},\\\n",
    "              running_acc: {running_acc/len(data_loader):.4f}, acc: {acc:.4f},\\\n",
    "              running_nmi: {running_nmi/len(data_loader):.4f}, nmi: {nmi:.4f}\")\n",
    "        # Store data to file\n",
    "        writer.writerow([epoch+1, running_loss/len(data_loader), running_acc/len(data_loader), acc, running_nmi/len(data_loader), nmi])\n",
    "\n",
    "def unsupervised_clustering_accuracy(y_true: torch.Tensor, y_pred: torch.Tensor, C: int=121) -> float:\n",
    "    \"\"\"\n",
    "    Computes the unsupervised clustering accuracy between two clusterings.\n",
    "    Uses the Hungarian algorithm to find the best matching between true and predicted labels.\n",
    "\n",
    "    Args:\n",
    "        y_true: true cluster labels as a 1D torch.Tensor\n",
    "        y_pred: predicted cluster labels as a 1D torch.Tensor\n",
    "        C:      number of classes\n",
    "\n",
    "    Returns:\n",
    "        accuracy: unsupervised clustering accuracy as a float\n",
    "    \"\"\"\n",
    "    # Create confusion matrix\n",
    "    cm = confusion_matrix(y_pred, y_true, labels=list(range(C)))\n",
    "\n",
    "    # Compute best matching between true and predicted labels using the Hungarian algorithm\n",
    "    _, col_ind = linear_sum_assignment(-cm)\n",
    "\n",
    "    # Reassign labels for the predicted clusters\n",
    "    y_pred_reassigned = torch.tensor(col_ind)[y_pred.long()]\n",
    "\n",
    "    # Compute accuracy as the percentage of correctly classified samples\n",
    "    acc = accuracy_score(y_true, y_pred_reassigned)\n",
    "\n",
    "    return acc\n",
    "\n",
    "\n",
    "def test_classifier(model, data_loader: DataLoader) -> None:\n",
    "    \"\"\"\n",
    "    Testing a classifier given the model and a test set.\n",
    "\n",
    "    Args:\n",
    "        model: Neural network model to train.\n",
    "        data_loader: PyTorch data loader containing the test data.\n",
    "    \n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    \n",
    "    # Disable gradient computation, not needed for inference\n",
    "    model.eval()\n",
    "    # Initialize tensors for storing true and predicted labels\n",
    "    y_true = torch.zeros(len(data_loader.dataset))\n",
    "    y_pred = torch.zeros(len(data_loader.dataset))\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Iterate over the mini-batches in the data loader\n",
    "        for i, data in enumerate(data_loader):\n",
    "            # Get the inputs and true labels for the mini-batch and reshape\n",
    "            inputs, labels_true = data\n",
    "            \n",
    "            # # TODO flattening should be done in the feed forward model, else statement should be removed\n",
    "            # inputs = inputs.view(inputs.size(0), -1)\n",
    "            \n",
    "            # Forward pass through the model to get predicted labels\n",
    "            labels_pred = F.softmax(model(inputs), dim=1)\n",
    "\n",
    "            # Store predicted and true labels in tensors\n",
    "            y_pred[i*len(labels_true):(i+1)*len(labels_true)] = torch.argmax(labels_pred, dim=1)\n",
    "            y_true[i*len(labels_true):(i+1)*len(labels_true)] = labels_true\n",
    "\n",
    "    # Compute unsupervised clustering accuracy score\n",
    "    acc = unsupervised_clustering_accuracy(y_true, y_pred)\n",
    "\n",
    "    print(f\"\\nThe unsupervised clustering accuracy score of the classifier is: {acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "from archt import get_model\n",
    "\n",
    "# Information Maximizing Self-Augmented Training\n",
    "from IMSAT import regularized_information_maximization\n",
    "\n",
    "# Invariant Information Clustering\n",
    "from IIC import invariant_information_clustering\n",
    "\n",
    "from datasets.dataset_classes import NDSBDataset, MNISTDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Create the train and test datasets\n",
    "train_dataset = NDSBDataset(train=True, augment_data=True)\n",
    "test_dataset  = NDSBDataset(train=False)\n",
    "\n",
    "# Create the train and test data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader  = DataLoader(test_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model specifications: CNN(\n",
      "  (conv1): Conv2d(1, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv3): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv_bn1): BatchNorm2d(16, eps=2e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv_bn3): BatchNorm2d(16, eps=2e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=183184, out_features=1024, bias=True)\n",
      "  (fc3): Linear(in_features=1024, out_features=121, bias=True)\n",
      "  (fc_bn1): BatchNorm1d(1024, eps=2e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU()\n",
      ")\n",
      "Epoch 1 loss: -0.9236,              running_acc: 0.1985, acc: 0.1574,              running_nmi: 0.4267, nmi: 0.2435\n",
      "Epoch 2 loss: -1.1152,              running_acc: 0.2153, acc: 0.1734,              running_nmi: 0.4608, nmi: 0.2667\n",
      "Epoch 3 loss: -1.2357,              running_acc: 0.2330, acc: 0.1953,              running_nmi: 0.4933, nmi: 0.2877\n",
      "Epoch 4 loss: -1.3292,              running_acc: 0.2389, acc: 0.2007,              running_nmi: 0.5049, nmi: 0.2958\n",
      "Epoch 5 loss: -1.3740,              running_acc: 0.2462, acc: 0.2043,              running_nmi: 0.5163, nmi: 0.3012\n",
      "\n",
      "The unsupervised clustering accuracy score of the classifier is: 0.2129483122362869\n"
     ]
    }
   ],
   "source": [
    "# Initialize model\n",
    "model = get_model(\"cnn\").to(device)\n",
    "\n",
    "# Initialize loss function, and optimizer\n",
    "criterion = invariant_information_clustering\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "# Store metadata to .log file\n",
    "logger = logging.getLogger(__name__)\n",
    "# Set the logging level\n",
    "logger.setLevel(logging.INFO)\n",
    "# Add handler to the logger\n",
    "logger.addHandler(logging.FileHandler(f'logs/{datetime.now().strftime(\"%Y-%m-%d-%H-%M\")}.log'))\n",
    "\n",
    "# Write metadata to .log file\n",
    "logger.info(f'Optimization criterion: {criterion.__name__}')\n",
    "logger.info(f'Learning rate: {lr}')\n",
    "logger.info(f'Number of epochs: {num_epochs}')\n",
    "logger.info(f'Batch size: {batch_size}')\n",
    "logger.info(f'Optimizer: {optimizer}')\n",
    "logger.info(f'Model: {model}')\n",
    "\n",
    "# Train the model\n",
    "train(model, train_loader, criterion, optimizer, num_epochs)\n",
    "\n",
    "# Close data file\n",
    "f.close()\n",
    "\n",
    "# Test model\n",
    "model.cpu()\n",
    "\n",
    "test_classifier(model, test_loader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "bd76f8a6c1bf20c479bc66ca0d5112f0d137b0d1ca4c89ae4addcd63596be2d9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

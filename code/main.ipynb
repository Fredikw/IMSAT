{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Libraries\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "from typing import Callable\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.init as init\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"\"\"\n",
    "\n",
    "# \"\"\"\n",
    "\n",
    "# from IMSAT import NeuralNet, regularized_information_maximization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Setting the hyperparameters\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "num_epochs: int = 20\n",
    "batch_size: int = 250    # Should be set to a power of 2.\n",
    "# Learning rate\n",
    "lr:         float = 0.002\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Data Preprocessing\n",
    "\n",
    "\"\"\"\n",
    "#TODO Preprocess the AILARON dataset to a suitable format.\n",
    "\n",
    "# #TODO Implement custome dataset for AILARON data. Should inherit from torch.utils.data.Dataset\n",
    "# class AILARONDataset(torchvision.Dataset):\n",
    "\n",
    "#     def __init__(self):\n",
    "#         # Load data\n",
    "#         pass\n",
    "\n",
    "#     def __getitem__(self, index):\n",
    "#         # TODO\n",
    "#         pass\n",
    "#     def __len__(self):\n",
    "#         # TODO\n",
    "#         pass \n",
    "\n",
    "# ailaron_train = AILARONDataset()\n",
    "# dataloader = DataLoader(dataset=ailaron_train, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Load MNIST dataset, normalizes data and transform to tensor.\n",
    "mnist_train = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=torchvision.transforms.ToTensor())\n",
    "mnist_test  = torchvision.datasets.MNIST(root='./data', train=False, download=False, transform=torchvision.transforms.ToTensor())\n",
    "\n",
    "\n",
    "# Create a subset of the MNIST dataset with the first 100 examples\n",
    "mnist_train_subset = torch.utils.data.Subset(mnist_train, range(3000))\n",
    "mnist_test_subset  = torch.utils.data.Subset(mnist_test, range(32))\n",
    "\n",
    "# # Get a random image from the dataset\n",
    "# image, label = mnist_train[np.random.randint(0, len(mnist_train))]\n",
    "\n",
    "# # Plot the image\n",
    "# plt.imshow(image[0], cmap='gray')\n",
    "# plt.title(f'Label: {label}')\n",
    "# plt.show()\n",
    "\n",
    "# Create DataLoader\n",
    "train_loader = DataLoader(mnist_train_subset, batch_size=batch_size, shuffle=True)\n",
    "test_loader  = DataLoader(mnist_test, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Libraries\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.init as init\n",
    "\n",
    "\"\"\"\n",
    "Settings\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Trade-off parameter for mutual information and smooth regularization\n",
    "lam:        float = 0.1\n",
    "\n",
    "\"\"\"\n",
    "Multi-output probabilistic classifier that maps similar inputs into similar representations.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        \n",
    "        # Add first fully connected layer with 28 * 28 = 784 input neurons and 1200 output neurons\n",
    "        self.fc1 = nn.Linear(28 * 28, 1200)\n",
    "        # Initialize the weights of the first fully connected layer using the He normal initialization\n",
    "        init.kaiming_normal_(self.fc1.weight, nonlinearity='relu')\n",
    "        # Add first batch normalization layer with 1200 neurons and epsilon = 2e-5\n",
    "        self.bn1   = nn.BatchNorm1d(1200, eps=2e-5)\n",
    "        self.bn1_F = nn.BatchNorm1d(1200, eps=2e-5, affine=False)\n",
    "        # Add first ReLU activation function\n",
    "        self.relu1 = nn.ReLU()\n",
    "        \n",
    "        self.fc2 = nn.Linear(1200, 1200)\n",
    "        init.kaiming_normal_(self.fc2.weight, nonlinearity='relu')\n",
    "        self.bn2   = nn.BatchNorm1d(1200, eps=2e-5)\n",
    "        self.bn2_F = nn.BatchNorm1d(1200, eps=2e-5, affine=False)\n",
    "\n",
    "        self.relu2 = nn.ReLU()\n",
    "        \n",
    "        # Add output layer of size 10 \n",
    "        self.fc3 = nn.Linear(1200, 10)\n",
    "        init.kaiming_normal_(self.fc3.weight, nonlinearity='linear')\n",
    "        \n",
    "    # Define the forward pass through the network\n",
    "    def forward(self, x):\n",
    "        # Pass the input through the first fully connected layer\n",
    "        x = self.fc1(x)\n",
    "        # Pass the output of the first fully connected layer through the first batch normalization layer\n",
    "        x = self.bn1(x)\n",
    "        # Pass the output of the first batch normalization layer through the first ReLU activation function\n",
    "        x = self.relu1(x)\n",
    "        \n",
    "        x = self.fc2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.relu2(x)\n",
    "        \n",
    "        x = self.fc3(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "\"\"\"\n",
    "Approximating the marginal distribution\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "def mariginal_distribution(conditionals: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Approximates the mariginal probability according to Eq (15).\n",
    "    \n",
    "    Args:\n",
    "        conditionals: conditional probabilities\n",
    "\n",
    "    Returns\n",
    "        An approximation of mariginal probabilities.\n",
    "    \"\"\"\n",
    "\n",
    "    # Calculate the sums for each columns.\n",
    "    return torch.sum(conditionals, dim=0) / conditionals.size()[0]\n",
    "\n",
    "\"\"\"\n",
    "Mutual Information\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "def shannon_entropy(probabilities: torch.Tensor) -> float:\n",
    "    \"\"\"\n",
    "    Calculates the Shannon entropy of a set of probabilities.\n",
    "    \n",
    "    Args:\n",
    "        probabilities:\n",
    "    \n",
    "    Returns:\n",
    "        The Shannon entropy\n",
    "    \"\"\"\n",
    "\n",
    "    return -torch.sum(probabilities * torch.log(probabilities))\n",
    "\n",
    "\n",
    "def mutual_information(mariginals: torch.Tensor, conditionals: torch.Tensor) -> float:\n",
    "    \"\"\"\n",
    "    Calculate the mutual information between two discrete random variables. According to Eq. (7).\n",
    "    \n",
    "    Args:\n",
    "        mariginals: Mariginal probabilities of X.\n",
    "        conditionals: Conditional probabilities, X|Y.\n",
    "    \n",
    "    Returns:\n",
    "        The mutual information between the two random variables.\n",
    "    \"\"\"\n",
    "\n",
    "    # TODO Remove\n",
    "    mu: float = 4\n",
    "    \n",
    "    marginal_entropy    = shannon_entropy(mariginals)\n",
    "    conditional_entropy = shannon_entropy(conditionals)\n",
    "\n",
    "    return mu * marginal_entropy - conditional_entropy\n",
    "\n",
    "\"\"\"\n",
    "Self-Augmented Training (SAT)\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "def self_augmented_training(model: NeuralNet, X: torch.Tensor, Y: torch.Tensor, eps: float = 1.0, ksi: float = 1e1, num_iters: int = 1) -> float:\n",
    "    \"\"\"\n",
    "    Self Augmented Training by Virtual Adversarial Training.\n",
    "    \n",
    "    Args:\n",
    "        model: multi-output probabilistic classifier. \n",
    "        X: Input samples.\n",
    "        Y: output when applying model on x.\n",
    "        eps: Perturbation size.\n",
    "        ksi: A small constant used for computing the finite difference approximation of the KL divergence.\n",
    "        num_iters: The number of iterations to use for computing the perturbation.\n",
    "\n",
    "    Returns:\n",
    "        The total loss (sum of cross-entropy loss on original input and perturbed input) for the batch.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    \"\"\"\n",
    "    Virtual Adversarial Training\n",
    "    \"\"\"\n",
    "    # Initial perturbation with the same shape as input samples.\n",
    "    r = torch.randn_like(X, requires_grad=True)\n",
    "    # Normalize the values along the second dimensions, i.e. sum along the columns equal one. \n",
    "    r = F.normalize(r, p=2, dim=1)\n",
    "\n",
    "    # # TODO Substitute the above for the following.    \n",
    "    # # Initialize a tensor r with the same shape as input samples, normalize the values in r across the second dimension (columns)\n",
    "    # r = torch.randn_like(X, requires_grad=True).renorm(2, 0, 1)\n",
    "\n",
    "    # TODO Remove\n",
    "    Y = model(X)\n",
    "\n",
    "    for i in range(num_iters):\n",
    "\n",
    "        # # Compute the discrete representation of the perturbed datapoints.\n",
    "        # Y_p = F.softmax(model(X + r * ksi), dim=1)\n",
    "\n",
    "        # TODO Remove\n",
    "        # # Compute the discrete representation of the perturbed datapoints.\n",
    "        Y_p = model(X + r * ksi)\n",
    "   \n",
    "        # Compute the KL divergence between the probabilities\n",
    "        kl_div = F.kl_div(Y.log(), Y_p, reduction='batchmean')\n",
    "        # Backward pass through the model and compute gradients\n",
    "        kl_div.backward(retain_graph=True)\n",
    "\n",
    "        # Set the perturbation as the gradient of the KL divergence w.r.t. r\n",
    "        r = r.grad\n",
    "        # Normalize the values along the second dimensions, i.e. sum along the columns equal one. \n",
    "        # r = F.normalize(r, p=2, dim=1)\n",
    "        print(r)\n",
    "\n",
    "    vad = r * eps\n",
    "\n",
    "    \"\"\"\n",
    "    Self Augmented Training\n",
    "    \"\"\"\n",
    "\n",
    "    Y_p = F.softmax(model(X + vad), dim=1)\n",
    "\n",
    "    loss = F.kl_div(Y.log(), Y_p, reduction='batchmean')\n",
    "    \n",
    "    return loss\n",
    "\n",
    "def regularized_information_maximization(model: NeuralNet, X: torch.Tensor, Y: torch.Tensor) -> float():\n",
    "    \"\"\"\n",
    "    Computes the loss using regularized information maximization.\n",
    "\n",
    "    Args:\n",
    "        model: multi-output probabilistic classifier. \n",
    "        X: Input samples.\n",
    "        Y: output when applying model on x.\n",
    "\n",
    "    Returns:\n",
    "        The loss given by mutual information and the regularization penalty.\n",
    "    \"\"\"\n",
    "\n",
    "    conditionals = Y\n",
    "    mariginals   = mariginal_distribution(Y)\n",
    "\n",
    "    I = mutual_information(mariginals, conditionals)\n",
    "\n",
    "    R_sat = self_augmented_training(model, X, Y)\n",
    "\n",
    "    return R_sat - lam * I"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Training the model\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Define the training function\n",
    "def train(model: NeuralNet, train_loader: DataLoader, criterion: Callable, optimizer: torch.optim, num_epochs: int):\n",
    "    \"\"\"\n",
    "    Trains a given model using the provided training data, optimizer and loss criterion for a given number of epochs.\n",
    "\n",
    "    Args:\n",
    "        model: Neural network model to train.\n",
    "        train_loader: PyTorch data loader containing the training data.\n",
    "        criterion: Loss criterion used for training the model.\n",
    "        optimizer: Optimizer used to update the model's parameters.\n",
    "        num_epochs: Number of epochs to train the model.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # Loop over the epochs\n",
    "    for epoch in range(num_epochs):\n",
    "        \n",
    "        # Initialize running loss for the epoch\n",
    "        running_loss = 0.0\n",
    "        \n",
    "        # Loop over the mini-batches in the data loader\n",
    "        for _, data in enumerate(train_loader):\n",
    "        \n",
    "            # Get the inputs and labels for the mini-batch and reshape\n",
    "            inputs, _ = data\n",
    "            inputs    = inputs.view(-1, 28*28)\n",
    "        \n",
    "            # Zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "        \n",
    "            # Forward pass through the model\n",
    "            outputs = F.softmax(model(inputs), dim=1)\n",
    "        \n",
    "            # Compute the loss\n",
    "            loss = criterion(model, inputs, outputs)\n",
    "            # Backward pass through the model and compute gradients\n",
    "            loss.backward()\n",
    "        \n",
    "            # Update the weights\n",
    "            optimizer.step()\n",
    "\n",
    "            # Accumulate the loss for the mini-batch\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        # Compute the average loss for the epoch and print\n",
    "        print(f\"Epoch {epoch+1} loss: {running_loss/len(train_loader)}\")\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Evaluation Metric\n",
    "\n",
    "\"\"\"\n",
    "# TODO consider including Normalized Information Score as an evaluation metric.\n",
    "\n",
    "def unsupervised_clustering_accuracy(y_true: torch.Tensor, y_pred: torch.Tensor) -> float:\n",
    "    \"\"\"\n",
    "    Computes the unsupervised clustering accuracy between two clusterings.\n",
    "    Uses the Hungarian algorithm to find the best matching between true and predicted labels.\n",
    "\n",
    "    Args:\n",
    "        y_true: true cluster labels as a 1D torch.Tensor\n",
    "        y_pred: predicted cluster labels as a 1D torch.Tensor\n",
    "\n",
    "    Returns:\n",
    "        accuracy: unsupervised clustering accuracy as a float\n",
    "    \"\"\"\n",
    "    # Create confusion matrix\n",
    "    cm = confusion_matrix(y_pred, y_true)\n",
    "\n",
    "    # Compute best matching between true and predicted labels using the Hungarian algorithm\n",
    "    _, col_ind = linear_sum_assignment(-cm)\n",
    "\n",
    "    # Reassign labels for the predicted clusters\n",
    "    y_pred_reassigned = torch.tensor(col_ind)[y_pred.long()]\n",
    "\n",
    "    # Compute accuracy as the percentage of correctly classified samples\n",
    "    acc = accuracy_score(y_true, y_pred_reassigned)\n",
    "\n",
    "    return acc\n",
    "\n",
    "\"\"\"\n",
    "Testing\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "def test_classifier(model: NeuralNet, test_loader: DataLoader) -> None:\n",
    "    \"\"\"\n",
    "    Testing a classifier given the model and a test set.\n",
    "\n",
    "    Args:\n",
    "        model: Neural network model to train.\n",
    "        test_loader: PyTorch data loader containing the test data.\n",
    "    \n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    \n",
    "    # Disable gradient computation, as we don't need it for inference\n",
    "    model.eval()\n",
    "    # Initialize tensors for true and predicted labels\n",
    "    y_true = torch.zeros(len(test_loader.dataset))\n",
    "    y_pred = torch.empty(len(test_loader.dataset))\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Iterate over the mini-batches in the data loader\n",
    "        for i, data in enumerate(test_loader):\n",
    "            # Get the inputs and true labels for the mini-batch and reshape\n",
    "            inputs, labels_true = data\n",
    "            inputs = inputs.view(-1, 28*28)\n",
    "\n",
    "            # Forward pass through the model to get predicted labels\n",
    "            labels_pred = F.softmax(model(inputs), dim=1)\n",
    "\n",
    "            # Store predicted and true labels in tensors\n",
    "            y_pred[i*len(labels_true):(i+1)*len(labels_true)] = torch.argmax(labels_pred, dim=1)\n",
    "            y_true[i*len(labels_true):(i+1)*len(labels_true)] = labels_true\n",
    "\n",
    "    # Compute unsupervised clustering accuracy score\n",
    "    acc = unsupervised_clustering_accuracy(y_true, y_pred)\n",
    "\n",
    "    print(f\"\\nThe unsupervised clustering accuracy score of the classifier is: {acc}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fredikw\\AppData\\Local\\Temp\\ipykernel_16940\\2063743182.py:176: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at aten\\src\\ATen/core/TensorBody.h:485.)\n",
      "  r = r.grad\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for *: 'NoneType' and 'float'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [70], line 11\u001b[0m\n\u001b[0;32m      8\u001b[0m optimizer \u001b[39m=\u001b[39m optim\u001b[39m.\u001b[39mAdam(model\u001b[39m.\u001b[39mparameters(), lr\u001b[39m=\u001b[39mlr)\n\u001b[0;32m     10\u001b[0m \u001b[39m# Train the model\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m train(model, train_loader, criterion, optimizer, num_epochs)\n\u001b[0;32m     12\u001b[0m \u001b[39m# Test model\u001b[39;00m\n\u001b[0;32m     13\u001b[0m test_classifier(model, test_loader)\n",
      "Cell \u001b[1;32mIn [69], line 41\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, train_loader, criterion, optimizer, num_epochs)\u001b[0m\n\u001b[0;32m     38\u001b[0m outputs \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39msoftmax(model(inputs), dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m     40\u001b[0m \u001b[39m# Compute the loss\u001b[39;00m\n\u001b[1;32m---> 41\u001b[0m loss \u001b[39m=\u001b[39m criterion(model, inputs, outputs)\n\u001b[0;32m     42\u001b[0m \u001b[39m# Backward pass through the model and compute gradients\u001b[39;00m\n\u001b[0;32m     43\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n",
      "Cell \u001b[1;32mIn [68], line 211\u001b[0m, in \u001b[0;36mregularized_information_maximization\u001b[1;34m(model, X, Y)\u001b[0m\n\u001b[0;32m    207\u001b[0m mariginals   \u001b[39m=\u001b[39m mariginal_distribution(Y)\n\u001b[0;32m    209\u001b[0m I \u001b[39m=\u001b[39m mutual_information(mariginals, conditionals)\n\u001b[1;32m--> 211\u001b[0m R_sat \u001b[39m=\u001b[39m self_augmented_training(model, X, Y)\n\u001b[0;32m    213\u001b[0m \u001b[39mreturn\u001b[39;00m R_sat \u001b[39m-\u001b[39m lam \u001b[39m*\u001b[39m I\n",
      "Cell \u001b[1;32mIn [68], line 181\u001b[0m, in \u001b[0;36mself_augmented_training\u001b[1;34m(model, X, Y, eps, ksi, num_iters)\u001b[0m\n\u001b[0;32m    177\u001b[0m     \u001b[39m# Normalize the values along the second dimensions, i.e. sum along the columns equal one. \u001b[39;00m\n\u001b[0;32m    178\u001b[0m     \u001b[39m# r = F.normalize(r, p=2, dim=1)\u001b[39;00m\n\u001b[0;32m    179\u001b[0m     \u001b[39mprint\u001b[39m(r)\n\u001b[1;32m--> 181\u001b[0m vad \u001b[39m=\u001b[39m r \u001b[39m*\u001b[39;49m eps\n\u001b[0;32m    183\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    184\u001b[0m \u001b[39mSelf Augmented Training\u001b[39;00m\n\u001b[0;32m    185\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    187\u001b[0m Y_p \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39msoftmax(model(X \u001b[39m+\u001b[39m vad), dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n",
      "\u001b[1;31mTypeError\u001b[0m: unsupported operand type(s) for *: 'NoneType' and 'float'"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Initialize the model, loss function, and optimizer\n",
    "model     = NeuralNet()\n",
    "criterion = regularized_information_maximization\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "# Train the model\n",
    "train(model, train_loader, criterion, optimizer, num_epochs)\n",
    "# Test model\n",
    "test_classifier(model, test_loader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "bd76f8a6c1bf20c479bc66ca0d5112f0d137b0d1ca4c89ae4addcd63596be2d9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Libraries\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import csv\n",
    "from datetime import datetime\n",
    "from typing import Callable\n",
    "\n",
    "import logging\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "from sklearn.metrics import accuracy_score, balanced_accuracy_score, confusion_matrix\n",
    "from sklearn.metrics.cluster import normalized_mutual_info_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of available devices: 1\n",
      " Device name: NVIDIA A100 80GB PCIe\n",
      " Total GPU memory device 0: 79.20 GB\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Setting generic hyperparameters\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "num_epochs: int = 40\n",
    "batch_size: int = 256 # Should be set to a power of 2.\n",
    "# Learning rate\n",
    "lr:         float = 1e-4 # Learning rate used in the IIC paper: lr=1e-4.\n",
    "\n",
    "\"\"\"\n",
    "GPU utilization\n",
    "\n",
    "\"\"\"\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Specifications\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"Number of available devices: {torch.cuda.device_count()}\\n\",\n",
    "          f\"Device name: {torch.cuda.get_device_name(torch.cuda.current_device())}\\n\",\n",
    "          f\"Total GPU memory device 0: {torch.cuda.get_device_properties(0).total_memory/(1024**3):.2f} GB\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "44"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Store data to .csv file\n",
    "\n",
    "'''\n",
    "\n",
    "# open the file for writing\n",
    "f = open(f'logs/IIC_ten_classes_balanced/{datetime.now().strftime(\"%Y-%m-%d-%H-%M\")}.csv', 'w')\n",
    "# create a CSV writer object\n",
    "writer = csv.writer(f)\n",
    "# write the header row to the CSV file\n",
    "writer.writerow(['epoch', 'loss', 'running_acc', 'acc', 'running_nmi', 'nmi'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The ten classes considered in case 2\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "subset_classes = ['acantharia_protist',\n",
    "                  'chordate_type1',\n",
    "                  'copepod_calanoid_eucalanus',\n",
    "                  'copepod_cyclopoid_copilia',\n",
    "                  'ctenophore_cestid',\n",
    "                  'ctenophore_lobate',\n",
    "                  'diatom_chain_string',\n",
    "                  'echinoderm_larva_seastar_brachiolaria',\n",
    "                  'hydromedusae_haliscera',\n",
    "                  'radiolarian_chain']\n",
    "\n",
    "mapping_dict = {0: 0, 13: 1, 16: 2, 23: 3, 28: 4, 31: 5, 36: 6, 43: 7, 60: 8, 90: 9}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Unsupervised Machine Learning Framework\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "def train(model, data_loader: DataLoader, criterion: Callable, optimizer: torch.optim, num_epochs: int, num_classes: int=None) -> None:\n",
    "    \"\"\"\n",
    "    Trains a given model using the provided training data, optimizer and loss criterion for a given number of epochs.\n",
    "\n",
    "    Args:\n",
    "        model: Neural network model to train.\n",
    "        data_loader: PyTorch data loader containing the training data.\n",
    "        criterion: Loss criterion used for training the model.\n",
    "        optimizer: Optimizer used to update the model's parameters.\n",
    "        num_epochs: Number of epochs to train the model.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "\n",
    "        running_loss = 0.0\n",
    "        running_acc  = 0.0\n",
    "        running_nmi  = 0.0\n",
    "\n",
    "        # Initialize tensors for storing true and predicted labels\n",
    "        labels_true = torch.zeros(len(data_loader.dataset))\n",
    "        labels_pred = torch.zeros(len(data_loader.dataset))\n",
    "\n",
    "        # Loop over the mini-batches in the data loader\n",
    "        for i, data in enumerate(data_loader):\n",
    "        \n",
    "            # Get the inputs and labels for the mini-batch\n",
    "            inputs, labels = data\n",
    "\n",
    "            # Use GPU if available\n",
    "            inputs = inputs.to(device)\n",
    "\n",
    "            # Image augmentation\n",
    "            if data_loader.dataset.augment_data:\n",
    "                inputs_trans = torch.stack([data_loader.dataset.transform_list(input) for input in inputs])\n",
    "                # # Flatten input data for the feed forward model\n",
    "                # inputs       = [inputs.view(inputs.size(0), -1), inputs_trans.view(inputs_trans.size(0), -1)]\n",
    "                inputs       = [inputs, inputs_trans]\n",
    "            # else:\n",
    "                # inputs = inputs.view(inputs.size(0), -1)\n",
    "        \n",
    "            # Zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass through the model\n",
    "            if data_loader.dataset.augment_data:\n",
    "                outputs = [F.softmax(model(inputs[0]), dim=1), F.softmax(model(inputs[1]), dim=1)]\n",
    "            else:\n",
    "                outputs = F.softmax(model(inputs), dim=1)\n",
    "\n",
    "            # Set arguments for objective function\n",
    "            # kwargs = {key: value for key, value in locals().items() if key in criterion.__code__.co_varnames}\n",
    "            kwargs = {\"model\": model, \"inputs\": inputs, \"outputs\": outputs}\n",
    "            kwargs = {key: value for key, value in kwargs.items() if key in criterion.__code__.co_varnames}\n",
    "            \n",
    "            # Compute the loss\n",
    "            loss = criterion(**kwargs)\n",
    "            # Backward pass through the model and compute gradients\n",
    "            loss.backward()\n",
    "        \n",
    "            # Update the weights\n",
    "            optimizer.step()\n",
    "\n",
    "            # Accumulate the loss for the mini-batch\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            outputs = outputs[0] if data_loader.dataset.augment_data else outputs\n",
    "\n",
    "            running_acc  += unsupervised_clustering_accuracy(labels, torch.argmax(outputs.cpu(), dim=1),C=num_classes)\n",
    "            running_nmi  += normalized_mutual_info_score(labels, torch.argmax(outputs.cpu(), dim=1))\n",
    "\n",
    "            # Store predicted and true labels in tensors\n",
    "            labels_true[i*len(labels):(i+1)*len(labels)] = labels\n",
    "            labels_pred[i*len(labels):(i+1)*len(labels)] = torch.argmax(outputs, dim=1)\n",
    "\n",
    "        acc = unsupervised_clustering_accuracy(labels_true, labels_pred, C=num_classes)\n",
    "        nmi = normalized_mutual_info_score(labels_true, labels_pred)\n",
    "\n",
    "        # Compute the average loss and accuracy for the epoch and print\n",
    "        print(f\"Epoch {epoch+1} loss: {running_loss/len(data_loader):.4f},\\\n",
    "              running_acc: {running_acc/len(data_loader):.4f}, acc: {acc:.4f},\\\n",
    "              running_nmi: {running_nmi/len(data_loader):.4f}, nmi: {nmi:.4f}\")\n",
    "        # Store data to file\n",
    "        writer.writerow([epoch+1, running_loss/len(data_loader), running_acc/len(data_loader), acc, running_nmi/len(data_loader), nmi])\n",
    "\n",
    "def reassign(y_true: torch.Tensor, y_pred: torch.Tensor, C: int=None) -> float:\n",
    "    \n",
    "    # Create confusion matrix\n",
    "    cm = confusion_matrix(y_pred, y_true, labels=list(range(C)))\n",
    "\n",
    "    # Compute best matching between true and predicted labels using the Hungarian algorithm\n",
    "    _, col_ind = linear_sum_assignment(-cm)\n",
    "\n",
    "    # Reassign labels for the predicted clusters\n",
    "    y_pred_reassigned = torch.tensor(col_ind)[y_pred.long()]\n",
    "    \n",
    "    return y_pred_reassigned\n",
    "        \n",
    "def unsupervised_clustering_accuracy(y_true: torch.Tensor, y_pred: torch.Tensor, C: int=None) -> float:\n",
    "    \"\"\"\n",
    "    Computes the unsupervised clustering accuracy between two clusterings.\n",
    "    Uses the Hungarian algorithm to find the best matching between true and predicted labels.\n",
    "\n",
    "    Args:\n",
    "        y_true: true cluster labels as a 1D torch.Tensor\n",
    "        y_pred: predicted cluster labels as a 1D torch.Tensor\n",
    "        C:      number of classes\n",
    "\n",
    "    Returns:\n",
    "        accuracy: unsupervised clustering accuracy as a float\n",
    "    \"\"\"\n",
    "    \n",
    "    y_pred_reassigned = reassign(y_true, y_pred, C)\n",
    "\n",
    "    # Compute accuracy as the percentage of correctly classified samples\n",
    "    acc = accuracy_score(y_true, y_pred_reassigned)\n",
    "\n",
    "    return acc\n",
    "\n",
    "def unsupervised_balanced_clustering_accuracy(y_true: torch.Tensor, y_pred: torch.Tensor, C: int=None) -> float:\n",
    "    \"\"\"\n",
    "    Computes the unsupervised clustering accuracy between two clusterings.\n",
    "    Uses the Hungarian algorithm to find the best matching between true and predicted labels.\n",
    "\n",
    "    Args:\n",
    "        y_true: true cluster labels as a 1D torch.Tensor\n",
    "        y_pred: predicted cluster labels as a 1D torch.Tensor\n",
    "        C:      number of classes\n",
    "\n",
    "    Returns:\n",
    "        accuracy: unsupervised clustering accuracy as a float\n",
    "    \"\"\"\n",
    "    \n",
    "    y_pred_reassigned = reassign(y_true, y_pred, C)\n",
    "\n",
    "    # Compute accuracy as the percentage of correctly classified samples\n",
    "    acc = balanced_accuracy_score(y_true, y_pred_reassigned)\n",
    "\n",
    "    return acc\n",
    "\n",
    "\n",
    "def test_classifier(model, data_loader: DataLoader, num_classes: int) -> float:\n",
    "    \"\"\"\n",
    "    Testing a classifier given the model and a test set.\n",
    "\n",
    "    Args:\n",
    "        model: Neural network model to train.\n",
    "        data_loader: PyTorch data loader containing the test data.\n",
    "    \n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    \n",
    "    # Disable gradient computation, not needed for inference\n",
    "    model.eval()\n",
    "    # Initialize tensors for storing true and predicted labels\n",
    "    y_true = torch.zeros(len(data_loader.dataset))\n",
    "    y_pred = torch.zeros(len(data_loader.dataset))\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Iterate over the mini-batches in the data loader\n",
    "        for i, data in enumerate(data_loader):\n",
    "            # Get the inputs and true labels for the mini-batch and reshape\n",
    "            inputs, labels_true = data\n",
    "            \n",
    "            # Use GPU if available\n",
    "            inputs      = inputs.to(device)\n",
    "                                    \n",
    "            # # TODO flattening should be done in the feed forward model, else statement should be removed\n",
    "            # inputs = inputs.view(inputs.size(0), -1)\n",
    "            \n",
    "            # Forward pass through the model to get predicted labels\n",
    "            labels_pred = F.softmax(model(inputs), dim=1)\n",
    "\n",
    "            # Store predicted and true labels in tensors\n",
    "            y_pred[i*len(labels_true):(i+1)*len(labels_true)] = torch.argmax(labels_pred.cpu(), dim=1)\n",
    "            y_true[i*len(labels_true):(i+1)*len(labels_true)] = labels_true\n",
    "\n",
    "    # Compute unsupervised clustering accuracy score\n",
    "    acc = unsupervised_clustering_accuracy(y_true, y_pred, C=num_classes)\n",
    "    \n",
    "    acc_balanced = unsupervised_balanced_clustering_accuracy(y_true, y_pred, C=num_classes)\n",
    "\n",
    "    print(f\"\\nThe unsupervised clustering accuracy score of the classifier is: {acc}\")\n",
    "    \n",
    "    return acc, acc_balanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "from archt import get_model\n",
    "\n",
    "# Information Maximizing Self-Augmented Training\n",
    "from IMSAT import regularized_information_maximization\n",
    "\n",
    "# Invariant Information Clustering\n",
    "from IIC import invariant_information_clustering\n",
    "\n",
    "from datasets.dataset_classes import NDSBDataset, MNISTDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Create the train and test datasets\n",
    "train_dataset = NDSBDataset(train=True, augment_data=True)\n",
    "test_dataset  = NDSBDataset(train=False)\n",
    "\n",
    "# Create the train and test data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader  = DataLoader(test_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/cluster/home/fredikw/.local/lib/python3.9/site-packages/torchvision/models/inception.py:80: FutureWarning: The default weight initialization of inception_v3 will be changed in future releases of torchvision. If you wish to keep the old behavior (which leads to long initialization times due to scipy/scipy#11299), please set init_weights=True.\n",
      "  warnings.warn('The default weight initialization of inception_v3 will be changed in future releases of '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model specifications: Inception3(\n",
      "  (Conv2d_1a_3x3): BasicConv2d(\n",
      "    (conv): Conv2d(1, 32, kernel_size=(3, 3), stride=(2, 2), bias=False)\n",
      "    (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (Conv2d_2a_3x3): BasicConv2d(\n",
      "    (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
      "    (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (Conv2d_2b_3x3): BasicConv2d(\n",
      "    (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (maxpool1): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (Conv2d_3b_1x1): BasicConv2d(\n",
      "    (conv): Conv2d(64, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn): BatchNorm2d(80, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (Conv2d_4a_3x3): BasicConv2d(\n",
      "    (conv): Conv2d(80, 192, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
      "    (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (maxpool2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (Mixed_5b): InceptionA(\n",
      "    (branch1x1): BasicConv2d(\n",
      "      (conv): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (branch5x5_1): BasicConv2d(\n",
      "      (conv): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn): BatchNorm2d(48, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (branch5x5_2): BasicConv2d(\n",
      "      (conv): Conv2d(48, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)\n",
      "      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (branch3x3dbl_1): BasicConv2d(\n",
      "      (conv): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (branch3x3dbl_2): BasicConv2d(\n",
      "      (conv): Conv2d(64, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (branch3x3dbl_3): BasicConv2d(\n",
      "      (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (branch_pool): BasicConv2d(\n",
      "      (conv): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (Mixed_5c): InceptionA(\n",
      "    (branch1x1): BasicConv2d(\n",
      "      (conv): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (branch5x5_1): BasicConv2d(\n",
      "      (conv): Conv2d(256, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn): BatchNorm2d(48, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (branch5x5_2): BasicConv2d(\n",
      "      (conv): Conv2d(48, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)\n",
      "      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (branch3x3dbl_1): BasicConv2d(\n",
      "      (conv): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (branch3x3dbl_2): BasicConv2d(\n",
      "      (conv): Conv2d(64, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (branch3x3dbl_3): BasicConv2d(\n",
      "      (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (branch_pool): BasicConv2d(\n",
      "      (conv): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (Mixed_5d): InceptionA(\n",
      "    (branch1x1): BasicConv2d(\n",
      "      (conv): Conv2d(288, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (branch5x5_1): BasicConv2d(\n",
      "      (conv): Conv2d(288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn): BatchNorm2d(48, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (branch5x5_2): BasicConv2d(\n",
      "      (conv): Conv2d(48, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)\n",
      "      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (branch3x3dbl_1): BasicConv2d(\n",
      "      (conv): Conv2d(288, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (branch3x3dbl_2): BasicConv2d(\n",
      "      (conv): Conv2d(64, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (branch3x3dbl_3): BasicConv2d(\n",
      "      (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (branch_pool): BasicConv2d(\n",
      "      (conv): Conv2d(288, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (Mixed_6a): InceptionB(\n",
      "    (branch3x3): BasicConv2d(\n",
      "      (conv): Conv2d(288, 384, kernel_size=(3, 3), stride=(2, 2), bias=False)\n",
      "      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (branch3x3dbl_1): BasicConv2d(\n",
      "      (conv): Conv2d(288, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (branch3x3dbl_2): BasicConv2d(\n",
      "      (conv): Conv2d(64, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (branch3x3dbl_3): BasicConv2d(\n",
      "      (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), bias=False)\n",
      "      (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (Mixed_6b): InceptionC(\n",
      "    (branch1x1): BasicConv2d(\n",
      "      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (branch7x7_1): BasicConv2d(\n",
      "      (conv): Conv2d(768, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (branch7x7_2): BasicConv2d(\n",
      "      (conv): Conv2d(128, 128, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
      "      (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (branch7x7_3): BasicConv2d(\n",
      "      (conv): Conv2d(128, 192, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
      "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (branch7x7dbl_1): BasicConv2d(\n",
      "      (conv): Conv2d(768, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (branch7x7dbl_2): BasicConv2d(\n",
      "      (conv): Conv2d(128, 128, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
      "      (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (branch7x7dbl_3): BasicConv2d(\n",
      "      (conv): Conv2d(128, 128, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
      "      (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (branch7x7dbl_4): BasicConv2d(\n",
      "      (conv): Conv2d(128, 128, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
      "      (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (branch7x7dbl_5): BasicConv2d(\n",
      "      (conv): Conv2d(128, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
      "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (branch_pool): BasicConv2d(\n",
      "      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (Mixed_6c): InceptionC(\n",
      "    (branch1x1): BasicConv2d(\n",
      "      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (branch7x7_1): BasicConv2d(\n",
      "      (conv): Conv2d(768, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (branch7x7_2): BasicConv2d(\n",
      "      (conv): Conv2d(160, 160, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
      "      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (branch7x7_3): BasicConv2d(\n",
      "      (conv): Conv2d(160, 192, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
      "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (branch7x7dbl_1): BasicConv2d(\n",
      "      (conv): Conv2d(768, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (branch7x7dbl_2): BasicConv2d(\n",
      "      (conv): Conv2d(160, 160, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
      "      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (branch7x7dbl_3): BasicConv2d(\n",
      "      (conv): Conv2d(160, 160, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
      "      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (branch7x7dbl_4): BasicConv2d(\n",
      "      (conv): Conv2d(160, 160, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
      "      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (branch7x7dbl_5): BasicConv2d(\n",
      "      (conv): Conv2d(160, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
      "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (branch_pool): BasicConv2d(\n",
      "      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (Mixed_6d): InceptionC(\n",
      "    (branch1x1): BasicConv2d(\n",
      "      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (branch7x7_1): BasicConv2d(\n",
      "      (conv): Conv2d(768, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (branch7x7_2): BasicConv2d(\n",
      "      (conv): Conv2d(160, 160, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
      "      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (branch7x7_3): BasicConv2d(\n",
      "      (conv): Conv2d(160, 192, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
      "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (branch7x7dbl_1): BasicConv2d(\n",
      "      (conv): Conv2d(768, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (branch7x7dbl_2): BasicConv2d(\n",
      "      (conv): Conv2d(160, 160, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
      "      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (branch7x7dbl_3): BasicConv2d(\n",
      "      (conv): Conv2d(160, 160, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
      "      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (branch7x7dbl_4): BasicConv2d(\n",
      "      (conv): Conv2d(160, 160, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
      "      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (branch7x7dbl_5): BasicConv2d(\n",
      "      (conv): Conv2d(160, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
      "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (branch_pool): BasicConv2d(\n",
      "      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (Mixed_6e): InceptionC(\n",
      "    (branch1x1): BasicConv2d(\n",
      "      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (branch7x7_1): BasicConv2d(\n",
      "      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (branch7x7_2): BasicConv2d(\n",
      "      (conv): Conv2d(192, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
      "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (branch7x7_3): BasicConv2d(\n",
      "      (conv): Conv2d(192, 192, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
      "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (branch7x7dbl_1): BasicConv2d(\n",
      "      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (branch7x7dbl_2): BasicConv2d(\n",
      "      (conv): Conv2d(192, 192, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
      "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (branch7x7dbl_3): BasicConv2d(\n",
      "      (conv): Conv2d(192, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
      "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (branch7x7dbl_4): BasicConv2d(\n",
      "      (conv): Conv2d(192, 192, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
      "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (branch7x7dbl_5): BasicConv2d(\n",
      "      (conv): Conv2d(192, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
      "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (branch_pool): BasicConv2d(\n",
      "      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (AuxLogits): InceptionAux(\n",
      "    (conv0): BasicConv2d(\n",
      "      (conv): Conv2d(768, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (conv1): BasicConv2d(\n",
      "      (conv): Conv2d(128, 768, kernel_size=(5, 5), stride=(1, 1), bias=False)\n",
      "      (bn): BatchNorm2d(768, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (fc): Linear(in_features=768, out_features=1000, bias=True)\n",
      "  )\n",
      "  (Mixed_7a): InceptionD(\n",
      "    (branch3x3_1): BasicConv2d(\n",
      "      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (branch3x3_2): BasicConv2d(\n",
      "      (conv): Conv2d(192, 320, kernel_size=(3, 3), stride=(2, 2), bias=False)\n",
      "      (bn): BatchNorm2d(320, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (branch7x7x3_1): BasicConv2d(\n",
      "      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (branch7x7x3_2): BasicConv2d(\n",
      "      (conv): Conv2d(192, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
      "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (branch7x7x3_3): BasicConv2d(\n",
      "      (conv): Conv2d(192, 192, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
      "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (branch7x7x3_4): BasicConv2d(\n",
      "      (conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), bias=False)\n",
      "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (Mixed_7b): InceptionE(\n",
      "    (branch1x1): BasicConv2d(\n",
      "      (conv): Conv2d(1280, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn): BatchNorm2d(320, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (branch3x3_1): BasicConv2d(\n",
      "      (conv): Conv2d(1280, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (branch3x3_2a): BasicConv2d(\n",
      "      (conv): Conv2d(384, 384, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1), bias=False)\n",
      "      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (branch3x3_2b): BasicConv2d(\n",
      "      (conv): Conv2d(384, 384, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)\n",
      "      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (branch3x3dbl_1): BasicConv2d(\n",
      "      (conv): Conv2d(1280, 448, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn): BatchNorm2d(448, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (branch3x3dbl_2): BasicConv2d(\n",
      "      (conv): Conv2d(448, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (branch3x3dbl_3a): BasicConv2d(\n",
      "      (conv): Conv2d(384, 384, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1), bias=False)\n",
      "      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (branch3x3dbl_3b): BasicConv2d(\n",
      "      (conv): Conv2d(384, 384, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)\n",
      "      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (branch_pool): BasicConv2d(\n",
      "      (conv): Conv2d(1280, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (Mixed_7c): InceptionE(\n",
      "    (branch1x1): BasicConv2d(\n",
      "      (conv): Conv2d(2048, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn): BatchNorm2d(320, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (branch3x3_1): BasicConv2d(\n",
      "      (conv): Conv2d(2048, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (branch3x3_2a): BasicConv2d(\n",
      "      (conv): Conv2d(384, 384, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1), bias=False)\n",
      "      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (branch3x3_2b): BasicConv2d(\n",
      "      (conv): Conv2d(384, 384, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)\n",
      "      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (branch3x3dbl_1): BasicConv2d(\n",
      "      (conv): Conv2d(2048, 448, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn): BatchNorm2d(448, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (branch3x3dbl_2): BasicConv2d(\n",
      "      (conv): Conv2d(448, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (branch3x3dbl_3a): BasicConv2d(\n",
      "      (conv): Conv2d(384, 384, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1), bias=False)\n",
      "      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (branch3x3dbl_3b): BasicConv2d(\n",
      "      (conv): Conv2d(384, 384, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)\n",
      "      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (branch_pool): BasicConv2d(\n",
      "      (conv): Conv2d(2048, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      "  (fc): Linear(in_features=2048, out_features=10, bias=True)\n",
      ")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 loss: -0.0097,              running_acc: 0.2523, acc: 0.2286,              running_nmi: 0.1567, nmi: 0.1110\n",
      "Epoch 2 loss: -0.0631,              running_acc: 0.2815, acc: 0.2808,              running_nmi: 0.2231, nmi: 0.2069\n",
      "Epoch 3 loss: -0.1252,              running_acc: 0.2966, acc: 0.3005,              running_nmi: 0.2520, nmi: 0.2300\n",
      "Epoch 4 loss: -0.2081,              running_acc: 0.3281, acc: 0.3232,              running_nmi: 0.2758, nmi: 0.2634\n",
      "Epoch 5 loss: -0.2965,              running_acc: 0.3329, acc: 0.3300,              running_nmi: 0.2839, nmi: 0.2764\n",
      "Epoch 6 loss: -0.3707,              running_acc: 0.3270, acc: 0.3310,              running_nmi: 0.2948, nmi: 0.2942\n",
      "Epoch 7 loss: -0.4426,              running_acc: 0.3291, acc: 0.3232,              running_nmi: 0.3174, nmi: 0.3096\n",
      "Epoch 8 loss: -0.5258,              running_acc: 0.3232, acc: 0.3232,              running_nmi: 0.3409, nmi: 0.3337\n",
      "Epoch 9 loss: -0.5681,              running_acc: 0.3389, acc: 0.3291,              running_nmi: 0.3598, nmi: 0.3387\n",
      "Epoch 10 loss: -0.6291,              running_acc: 0.3595, acc: 0.3448,              running_nmi: 0.3950, nmi: 0.3727\n",
      "Epoch 11 loss: -0.6646,              running_acc: 0.3698, acc: 0.3517,              running_nmi: 0.4214, nmi: 0.3986\n",
      "Epoch 12 loss: -0.7169,              running_acc: 0.3587, acc: 0.3645,              running_nmi: 0.4153, nmi: 0.4014\n",
      "Epoch 13 loss: -0.7399,              running_acc: 0.3677, acc: 0.3704,              running_nmi: 0.4188, nmi: 0.4006\n",
      "Epoch 14 loss: -0.7686,              running_acc: 0.3834, acc: 0.3852,              running_nmi: 0.4289, nmi: 0.4146\n",
      "Epoch 15 loss: -0.7994,              running_acc: 0.4034, acc: 0.4020,              running_nmi: 0.4426, nmi: 0.4251\n",
      "Epoch 16 loss: -0.8140,              running_acc: 0.3974, acc: 0.3961,              running_nmi: 0.4530, nmi: 0.4319\n",
      "Epoch 17 loss: -0.8335,              running_acc: 0.3981, acc: 0.3892,              running_nmi: 0.4582, nmi: 0.4414\n",
      "Epoch 18 loss: -0.8873,              running_acc: 0.3973, acc: 0.3961,              running_nmi: 0.4721, nmi: 0.4557\n",
      "Epoch 19 loss: -0.9086,              running_acc: 0.4048, acc: 0.4069,              running_nmi: 0.4776, nmi: 0.4611\n",
      "Epoch 20 loss: -0.9444,              running_acc: 0.4268, acc: 0.4315,              running_nmi: 0.4781, nmi: 0.4649\n",
      "Epoch 21 loss: -0.9933,              running_acc: 0.4435, acc: 0.4463,              running_nmi: 0.4844, nmi: 0.4670\n",
      "Epoch 22 loss: -1.0176,              running_acc: 0.4590, acc: 0.4660,              running_nmi: 0.5061, nmi: 0.4930\n",
      "Epoch 23 loss: -1.0285,              running_acc: 0.4775, acc: 0.4670,              running_nmi: 0.5021, nmi: 0.4809\n",
      "Epoch 24 loss: -1.0481,              running_acc: 0.4759, acc: 0.4749,              running_nmi: 0.5233, nmi: 0.5041\n",
      "Epoch 25 loss: -1.0377,              running_acc: 0.4782, acc: 0.4818,              running_nmi: 0.5162, nmi: 0.4914\n",
      "Epoch 26 loss: -1.1276,              running_acc: 0.4928, acc: 0.5015,              running_nmi: 0.5315, nmi: 0.5137\n",
      "Epoch 27 loss: -1.0822,              running_acc: 0.4676, acc: 0.4749,              running_nmi: 0.5370, nmi: 0.5187\n",
      "Epoch 28 loss: -1.0839,              running_acc: 0.4641, acc: 0.4670,              running_nmi: 0.5222, nmi: 0.5072\n",
      "Epoch 29 loss: -1.0778,              running_acc: 0.4590, acc: 0.4591,              running_nmi: 0.5200, nmi: 0.5034\n",
      "Epoch 30 loss: -1.0934,              running_acc: 0.4574, acc: 0.4631,              running_nmi: 0.5219, nmi: 0.5088\n",
      "Epoch 31 loss: -1.1196,              running_acc: 0.4782, acc: 0.4680,              running_nmi: 0.5340, nmi: 0.5143\n",
      "Epoch 32 loss: -1.1309,              running_acc: 0.4708, acc: 0.4670,              running_nmi: 0.5295, nmi: 0.5195\n",
      "Epoch 33 loss: -1.1502,              running_acc: 0.4770, acc: 0.4690,              running_nmi: 0.5387, nmi: 0.5145\n",
      "Epoch 34 loss: -1.1320,              running_acc: 0.4956, acc: 0.4788,              running_nmi: 0.5318, nmi: 0.5035\n",
      "Epoch 35 loss: -1.1919,              running_acc: 0.4791, acc: 0.4847,              running_nmi: 0.5431, nmi: 0.5213\n",
      "Epoch 36 loss: -1.1968,              running_acc: 0.4728, acc: 0.4729,              running_nmi: 0.5463, nmi: 0.5285\n",
      "Epoch 37 loss: -1.1743,              running_acc: 0.4790, acc: 0.4709,              running_nmi: 0.5406, nmi: 0.5207\n",
      "Epoch 38 loss: -1.2147,              running_acc: 0.4957, acc: 0.4906,              running_nmi: 0.5480, nmi: 0.5253\n",
      "Epoch 39 loss: -1.2056,              running_acc: 0.5163, acc: 0.5222,              running_nmi: 0.5691, nmi: 0.5428\n",
      "Epoch 40 loss: -1.2065,              running_acc: 0.5182, acc: 0.5291,              running_nmi: 0.5689, nmi: 0.5509\n",
      "\n",
      "The unsupervised clustering accuracy score of the classifier is: 0.6076696165191741\n"
     ]
    }
   ],
   "source": [
    "# Initialize model\n",
    "model = get_model(\"inception_v3\", num_classes=10).to(device)\n",
    "\n",
    "# Initialize loss function, and optimizer\n",
    "criterion = invariant_information_clustering\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "# Store metadata to .log file\n",
    "logger = logging.getLogger(__name__)\n",
    "# Set the logging level\n",
    "logger.setLevel(logging.INFO)\n",
    "# Add handler to the logger\n",
    "logger.addHandler(logging.FileHandler(f'logs/IIC_ten_classes_balanced/{datetime.now().strftime(\"%Y-%m-%d-%H-%M\")}.log'))\n",
    "\n",
    "# Write metadata to .log file\n",
    "logger.info(f'Optimization criterion: {criterion.__name__}')\n",
    "logger.info(f'Learning rate: {lr}')\n",
    "logger.info(f'Number of epochs: {num_epochs}')\n",
    "logger.info(f'Batch size: {batch_size}')\n",
    "logger.info(f'Optimizer: {optimizer}')\n",
    "logger.info(f'Model: {model}')\n",
    "\n",
    "# Train the model\n",
    "train(model, train_loader, criterion, optimizer, num_epochs, num_classes=10)\n",
    "\n",
    "# Test model\n",
    "acc, acc_balanced = test_classifier(model, test_loader,num_classes=10)\n",
    "\n",
    "logger.info(f'Accuracy: {acc}')\n",
    "logger.info(f'Balanced Accuracy: {acc_balanced}')\n",
    "# Close data file\n",
    "f.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "bd76f8a6c1bf20c479bc66ca0d5112f0d137b0d1ca4c89ae4addcd63596be2d9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

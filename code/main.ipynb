{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Libraries\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "from typing import Callable\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from torch.utils.data import DataLoader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Settings\n",
    "\n",
    "\"\"\"\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "\"\"\"\n",
    "Setting generic hyperparameters\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "num_epochs: int = 5#20\n",
    "batch_size: int = 250   # Should be set to a power of 2.\n",
    "# Learning rate\n",
    "lr:         float = 0.002"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Use GPU if available, otherwise use CPU\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Unsupervised Machine Learning Framework\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "def train(model, train_loader: DataLoader, criterion: Callable, optimizer: torch.optim, num_epochs: int) -> None:\n",
    "    \"\"\"\n",
    "    Trains a given model using the provided training data, optimizer and loss criterion for a given number of epochs.\n",
    "\n",
    "    Args:\n",
    "        model: Neural network model to train.\n",
    "        train_loader: PyTorch data loader containing the training data.\n",
    "        criterion: Loss criterion used for training the model.\n",
    "        optimizer: Optimizer used to update the model's parameters.\n",
    "        num_epochs: Number of epochs to train the model.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # Loop over the epochs\n",
    "    for epoch in range(num_epochs):\n",
    "\n",
    "        # Initialize running loss for the epoch\n",
    "        running_loss = 0.0\n",
    "        running_acc  = 0.0\n",
    "\n",
    "        # Loop over the mini-batches in the data loader\n",
    "        for _, data in enumerate(train_loader):\n",
    "        \n",
    "            # Get the inputs and labels for the mini-batch and reshape\n",
    "            inputs, labels = data\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "        \n",
    "            # Zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass through the model\n",
    "            if train_loader.dataset.augment:\n",
    "                outputs = (F.softmax(model(inputs[0]), dim=1), F.softmax(model(inputs[1]), dim=1))\n",
    "            else:\n",
    "                outputs = F.softmax(model(inputs), dim=1)\n",
    "\n",
    "            # Set arguments for objective functions\n",
    "            kwargs = {\"model\": model, \"inputs\": inputs, \"outputs\": outputs}\n",
    "            kwargs = {key: value for key, value in kwargs.items() if key in criterion.__code__.co_varnames}\n",
    "                    \n",
    "            # Compute the loss\n",
    "            loss = criterion(**kwargs)\n",
    "            # Backward pass through the model and compute gradients\n",
    "            loss.backward()\n",
    "        \n",
    "            # Update the weights\n",
    "            optimizer.step()\n",
    "\n",
    "            # Accumulate the loss for the mini-batch\n",
    "            running_loss += loss.item()\n",
    "            # Accumulate the accuracy for the mini-batch\n",
    "\n",
    "            outputs = outputs[0] if train_loader.dataset.augment else outputs\n",
    "\n",
    "            running_acc  += unsupervised_clustering_accuracy(labels, torch.argmax(outputs, dim=1))\n",
    "\n",
    "        # Compute the average loss for the epoch and print\n",
    "        print(f\"Epoch {epoch+1} loss: {running_loss/len(train_loader):.4f}, ACC: {running_acc/len(train_loader):.4f}\")\n",
    "\n",
    "\n",
    "def unsupervised_clustering_accuracy(y_true: torch.Tensor, y_pred: torch.Tensor) -> float:\n",
    "    \"\"\"\n",
    "    Computes the unsupervised clustering accuracy between two clusterings.\n",
    "    Uses the Hungarian algorithm to find the best matching between true and predicted labels.\n",
    "\n",
    "    Args:\n",
    "        y_true: true cluster labels as a 1D torch.Tensor\n",
    "        y_pred: predicted cluster labels as a 1D torch.Tensor\n",
    "\n",
    "    Returns:\n",
    "        accuracy: unsupervised clustering accuracy as a float\n",
    "    \"\"\"\n",
    "    # Create confusion matrix\n",
    "    cm = confusion_matrix(y_pred, y_true)\n",
    "\n",
    "    # Compute best matching between true and predicted labels using the Hungarian algorithm\n",
    "    _, col_ind = linear_sum_assignment(-cm)\n",
    "\n",
    "    # Reassign labels for the predicted clusters\n",
    "    y_pred_reassigned = torch.tensor(col_ind)[y_pred.long()]\n",
    "\n",
    "    # Compute accuracy as the percentage of correctly classified samples\n",
    "    acc = accuracy_score(y_true, y_pred_reassigned)\n",
    "\n",
    "    return acc\n",
    "\n",
    "\n",
    "def test_classifier(model, test_loader: DataLoader) -> None:\n",
    "    \"\"\"\n",
    "    Testing a classifier given the model and a test set.\n",
    "\n",
    "    Args:\n",
    "        model: Neural network model to train.\n",
    "        test_loader: PyTorch data loader containing the test data.\n",
    "    \n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    \n",
    "    # Disable gradient computation, as we don't need it for inference\n",
    "    model.eval()\n",
    "    # Initialize tensors for true and predicted labels\n",
    "    y_true = torch.zeros(len(test_loader.dataset))\n",
    "    y_pred = torch.empty(len(test_loader.dataset))\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Iterate over the mini-batches in the data loader\n",
    "        for i, data in enumerate(test_loader):\n",
    "            # Get the inputs and true labels for the mini-batch and reshape\n",
    "            inputs, labels_true = data\n",
    "\n",
    "            # Forward pass through the model to get predicted labels\n",
    "            labels_pred = F.softmax(model(inputs), dim=1)\n",
    "\n",
    "            # Store predicted and true labels in tensors\n",
    "            y_pred[i*len(labels_true):(i+1)*len(labels_true)] = torch.argmax(labels_pred, dim=1)\n",
    "            y_true[i*len(labels_true):(i+1)*len(labels_true)] = labels_true\n",
    "\n",
    "    # Compute unsupervised clustering accuracy score\n",
    "    acc = unsupervised_clustering_accuracy(y_true, y_pred)\n",
    "\n",
    "    print(f\"\\nThe unsupervised clustering accuracy score of the classifier is: {acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Unsupervised Machine Learning Algorithms\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "#TODO Consider implementing models as classes.\n",
    "\n",
    "from archt import NeuralNet\n",
    "\n",
    "# Information Maximizing Self-Augmented Training\n",
    "from IMSAT import regularized_information_maximization\n",
    "\n",
    "# Invariant Information Clustering\n",
    "from IIC import invariant_information_clustering\n",
    "\n",
    "from data import MNISTDataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Create the train and test datasets\n",
    "train_dataset = MNISTDataset(train=True, augment=False)\n",
    "test_dataset  = MNISTDataset(train=False)\n",
    "\n",
    "# Create the train and test data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader  = DataLoader(test_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\fredikw\\Desktop\\NTNU\\M.Sc\\code\\ms-deep-learning-based-plankton-classification\\code\\main.ipynb Cell 7\u001b[0m in \u001b[0;36m1\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/fredikw/Desktop/NTNU/M.Sc/code/ms-deep-learning-based-plankton-classification/code/main.ipynb#X62sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m optimizer \u001b[39m=\u001b[39m optim\u001b[39m.\u001b[39mAdam(model\u001b[39m.\u001b[39mparameters(), lr\u001b[39m=\u001b[39mlr)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/fredikw/Desktop/NTNU/M.Sc/code/ms-deep-learning-based-plankton-classification/code/main.ipynb#X62sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39m# Train the model\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/fredikw/Desktop/NTNU/M.Sc/code/ms-deep-learning-based-plankton-classification/code/main.ipynb#X62sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m train(model, train_loader, criterion, optimizer, num_epochs)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/fredikw/Desktop/NTNU/M.Sc/code/ms-deep-learning-based-plankton-classification/code/main.ipynb#X62sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39m# Test model\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/fredikw/Desktop/NTNU/M.Sc/code/ms-deep-learning-based-plankton-classification/code/main.ipynb#X62sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m test_classifier(model, test_loader)\n",
      "\u001b[1;32mc:\\Users\\fredikw\\Desktop\\NTNU\\M.Sc\\code\\ms-deep-learning-based-plankton-classification\\code\\main.ipynb Cell 7\u001b[0m in \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/fredikw/Desktop/NTNU/M.Sc/code/ms-deep-learning-based-plankton-classification/code/main.ipynb#X62sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m running_acc  \u001b[39m=\u001b[39m \u001b[39m0.0\u001b[39m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/fredikw/Desktop/NTNU/M.Sc/code/ms-deep-learning-based-plankton-classification/code/main.ipynb#X62sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m \u001b[39m# Loop over the mini-batches in the data loader\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/fredikw/Desktop/NTNU/M.Sc/code/ms-deep-learning-based-plankton-classification/code/main.ipynb#X62sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m \u001b[39mfor\u001b[39;00m _, data \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(train_loader):\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/fredikw/Desktop/NTNU/M.Sc/code/ms-deep-learning-based-plankton-classification/code/main.ipynb#X62sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m \n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/fredikw/Desktop/NTNU/M.Sc/code/ms-deep-learning-based-plankton-classification/code/main.ipynb#X62sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m     \u001b[39m# Get the inputs and labels for the mini-batch and reshape\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/fredikw/Desktop/NTNU/M.Sc/code/ms-deep-learning-based-plankton-classification/code/main.ipynb#X62sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m     inputs, labels \u001b[39m=\u001b[39m data\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/fredikw/Desktop/NTNU/M.Sc/code/ms-deep-learning-based-plankton-classification/code/main.ipynb#X62sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m     inputs \u001b[39m=\u001b[39m inputs\u001b[39m.\u001b[39mto(device)\n",
      "File \u001b[1;32mc:\\Users\\fredikw\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:628\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    625\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    626\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    627\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 628\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[0;32m    629\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    630\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    631\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    632\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\Users\\fredikw\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:671\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    669\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    670\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 671\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    672\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[0;32m    673\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\fredikw\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:58\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     56\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     57\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 58\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     59\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     60\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Users\\fredikw\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:58\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     56\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     57\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 58\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     59\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     60\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Users\\fredikw\\Desktop\\NTNU\\M.Sc\\code\\ms-deep-learning-based-plankton-classification\\code\\data.py:51\u001b[0m, in \u001b[0;36mMNISTDataset.__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m     48\u001b[0m     \u001b[39mreturn\u001b[39;00m (squeeze(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmnist[index][\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mview(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m28\u001b[39m\u001b[39m*\u001b[39m\u001b[39m28\u001b[39m)), squeeze(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmnist_augmented[index][\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mview(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m28\u001b[39m\u001b[39m*\u001b[39m\u001b[39m28\u001b[39m))), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmnist[index][\u001b[39m1\u001b[39m]\n\u001b[0;32m     49\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     50\u001b[0m     \u001b[39m# Return the original image and label at the given index\u001b[39;00m\n\u001b[1;32m---> 51\u001b[0m     \u001b[39mreturn\u001b[39;00m  squeeze(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmnist[index][\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mview(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m,\u001b[39m28\u001b[39m\u001b[39m*\u001b[39m\u001b[39m28\u001b[39m)), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmnist[index][\u001b[39m1\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\fredikw\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchvision\\datasets\\mnist.py:145\u001b[0m, in \u001b[0;36mMNIST.__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m    142\u001b[0m img \u001b[39m=\u001b[39m Image\u001b[39m.\u001b[39mfromarray(img\u001b[39m.\u001b[39mnumpy(), mode\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mL\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    144\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransform \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 145\u001b[0m     img \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtransform(img)\n\u001b[0;32m    147\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtarget_transform \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    148\u001b[0m     target \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtarget_transform(target)\n",
      "File \u001b[1;32mc:\\Users\\fredikw\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchvision\\transforms\\transforms.py:135\u001b[0m, in \u001b[0;36mToTensor.__call__\u001b[1;34m(self, pic)\u001b[0m\n\u001b[0;32m    127\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, pic):\n\u001b[0;32m    128\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    129\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[0;32m    130\u001b[0m \u001b[39m        pic (PIL Image or numpy.ndarray): Image to be converted to tensor.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    133\u001b[0m \u001b[39m        Tensor: Converted image.\u001b[39;00m\n\u001b[0;32m    134\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 135\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mto_tensor(pic)\n",
      "File \u001b[1;32mc:\\Users\\fredikw\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchvision\\transforms\\functional.py:167\u001b[0m, in \u001b[0;36mto_tensor\u001b[1;34m(pic)\u001b[0m\n\u001b[0;32m    165\u001b[0m \u001b[39mif\u001b[39;00m pic\u001b[39m.\u001b[39mmode \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m1\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m    166\u001b[0m     img \u001b[39m=\u001b[39m \u001b[39m255\u001b[39m \u001b[39m*\u001b[39m img\n\u001b[1;32m--> 167\u001b[0m img \u001b[39m=\u001b[39m img\u001b[39m.\u001b[39;49mview(pic\u001b[39m.\u001b[39;49msize[\u001b[39m1\u001b[39;49m], pic\u001b[39m.\u001b[39;49msize[\u001b[39m0\u001b[39;49m], \u001b[39mlen\u001b[39;49m(pic\u001b[39m.\u001b[39;49mgetbands()))\n\u001b[0;32m    168\u001b[0m \u001b[39m# put it from HWC to CHW format\u001b[39;00m\n\u001b[0;32m    169\u001b[0m img \u001b[39m=\u001b[39m img\u001b[39m.\u001b[39mpermute((\u001b[39m2\u001b[39m, \u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m))\u001b[39m.\u001b[39mcontiguous()\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Initialize the model, loss function, and optimizer\n",
    "model     = NeuralNet().to(device)\n",
    "criterion = regularized_information_maximization\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "# Train the model\n",
    "train(model, train_loader, criterion, optimizer, num_epochs)\n",
    "# Test model\n",
    "test_classifier(model, test_loader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
